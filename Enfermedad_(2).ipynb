{
  "metadata": {
    "name": "Enfermedad",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgkF6AT_5ciw",
        "outputId": "d3bbe6c9-896e-4c19-c6c9-3665a378c761"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "RrbQeVqtqW0D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "dcp1mmoNqdPT",
        "outputId": "c61d3281-2fd4-47ec-e13a-42900dcd34d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Unable to find py4j in /content/spark-3.5.1-bin-hadoop3/python, your SPARK_HOME may not be configured correctly",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7c456d13e0cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.repl.eagerEval.enabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Property used to format output tables better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.5.1-bin-hadoop3/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ruta_base = \"file:/home/vagrant/\"\n",
        "data = spark.sparkContext.textFile(\"enfermedades.csv\").map(lambda x: x.split(\";\")).cache()\n",
        "data.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2qrADJNqnLz",
        "outputId": "263217c1-0c28-4137-f606-92315a859f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['DOCUMENTO_ANONIMIZADO',\n",
              "  'PERIODO',\n",
              "  'MES',\n",
              "  'RENAES',\n",
              "  'IPRESS',\n",
              "  'REGION',\n",
              "  'DEPARTAMENTO',\n",
              "  'PROVINCIA',\n",
              "  'DISTRITO',\n",
              "  'UBIGEO',\n",
              "  'CODIGO_DIAGNOSTICO',\n",
              "  'DIAGNOSTICOS',\n",
              "  'GRUPO_DIAGNOSTICOS',\n",
              "  'GRUPO_COBERTURA',\n",
              "  'SEXO',\n",
              "  'EDAD',\n",
              "  'TIPO_SEGURO',\n",
              "  'SERVICIO',\n",
              "  'FECHA_ATENCION',\n",
              "  'MONTO_BRUTO',\n",
              "  'FECHA_CORTE'],\n",
              " ['27549',\n",
              "  '2022',\n",
              "  '7',\n",
              "  '16918',\n",
              "  'INSTITUTO NACIONAL DE SALUD DEL NI�O - SAN BORJA',\n",
              "  'LIMA',\n",
              "  'Lima',\n",
              "  'Lima',\n",
              "  'Lima',\n",
              "  '150101.0',\n",
              "  'Z948',\n",
              "  'ESTADO DE TRASPLANTE DE M�DULA �SEA',\n",
              "  'TRASPLANTE DE PROGENITORES HEMATOPOY�TICOS',\n",
              "  'PAC',\n",
              "  'MASCULINO',\n",
              "  '5',\n",
              "  'SIS GRATUITO',\n",
              "  'AMBULATORIO',\n",
              "  '20220627',\n",
              "  '0.22',\n",
              "  '20230525'],\n",
              " ['31973',\n",
              "  '2022',\n",
              "  '6',\n",
              "  '1703',\n",
              "  'ELEAZAR GUZMAN BARRON',\n",
              "  'ANCASH',\n",
              "  'Ancash',\n",
              "  'Santa',\n",
              "  'Nuevo Chimbote',\n",
              "  '21809.0',\n",
              "  'E119',\n",
              "  'Diabetes mellitus no insulinodependiente, sin menci�n de complicaci�n (MODY)',\n",
              "  'ENFERMEDAD RARA O HUERFANA',\n",
              "  'ERH',\n",
              "  'MASCULINO',\n",
              "  '68',\n",
              "  'SIS GRATUITO',\n",
              "  'AMBULATORIO',\n",
              "  '20220425',\n",
              "  '13.46',\n",
              "  '20230525'],\n",
              " ['8629',\n",
              "  '2022',\n",
              "  '3',\n",
              "  '6216',\n",
              "  'INSTITUTO NACIONAL DE SALUD DEL NI�O - BRE�A',\n",
              "  'LIMA',\n",
              "  'Lima',\n",
              "  'Lima',\n",
              "  'Bre�a',\n",
              "  '150105.0',\n",
              "  'Q750',\n",
              "  'Craneosinostosis',\n",
              "  'ENFERMEDAD RARA O HUERFANA',\n",
              "  'ERH',\n",
              "  'MASCULINO',\n",
              "  '0',\n",
              "  'SIS GRATUITO',\n",
              "  'AMBULATORIO',\n",
              "  '20220126',\n",
              "  '10.69',\n",
              "  '20230525'],\n",
              " ['88030',\n",
              "  '2022',\n",
              "  '5',\n",
              "  '6219',\n",
              "  'HOSPITAL SAN JOSE',\n",
              "  'CALLAO',\n",
              "  'Callao',\n",
              "  'Callao',\n",
              "  'Carmen de La Legua',\n",
              "  '70103.0',\n",
              "  'E119',\n",
              "  'Diabetes mellitus no insulinodependiente, sin menci�n de complicaci�n (MODY)',\n",
              "  'ENFERMEDAD RARA O HUERFANA',\n",
              "  'ERH',\n",
              "  'FEMENINO',\n",
              "  '62',\n",
              "  'SIS GRATUITO',\n",
              "  'AMBULATORIO',\n",
              "  '20220325',\n",
              "  '6.0',\n",
              "  '20230525']]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApgV_bicqFkl",
        "outputId": "d04e8411-ec0a-4e09-d081-56c8057c8f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DOCUMENTO_ANONIMIZADO', 'PERIODO', 'MES', 'RENAES', 'IPRESS', 'REGION', 'DEPARTAMENTO', 'PROVINCIA', 'DISTRITO', 'UBIGEO', 'CODIGO_DIAGNOSTICO', 'DIAGNOSTICOS', 'GRUPO_DIAGNOSTICOS', 'GRUPO_COBERTURA', 'SEXO', 'EDAD', 'TIPO_SEGURO', 'SERVICIO', 'FECHA_ATENCION', 'MONTO_BRUTO', 'FECHA_CORTE']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Obtenemos la primera fila del RDD\n",
        "encabezado = data.first()\n",
        "\n",
        "# Imprimimos el encabezado\n",
        "print(encabezado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "LsodLQJVqFkl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType, IntegerType\n",
        "\n",
        "estructura = StructType([\n",
        "    StructField('documento_anonimizado', StringType(), True),\n",
        "    StructField('periodo', IntegerType(), True),\n",
        "    StructField('mes', IntegerType(), True),\n",
        "    StructField('renaes', StringType(), True),\n",
        "    StructField('ipress', StringType(), True),\n",
        "    StructField('region', StringType(), True),\n",
        "    StructField('departamento', StringType(), True),\n",
        "    StructField('provincia', StringType(), True),\n",
        "    StructField('distrito', StringType(), True),\n",
        "    StructField('ubigeo', StringType(), True),\n",
        "    StructField('codigo_diagnostico', StringType(), True),\n",
        "    StructField('diagnosticos', StringType(), True),\n",
        "    StructField('grupo_diagnosticos', StringType(), True),\n",
        "    StructField('grupo_cobertura', StringType(), True),\n",
        "    StructField('sexo', StringType(), True),\n",
        "    StructField('edad', IntegerType(), True),\n",
        "    StructField('tipo_seguro', StringType(), True),\n",
        "    StructField('servicio', StringType(), True),\n",
        "    StructField('fecha_atencion', DateType(), True),\n",
        "    StructField('monto_bruto', FloatType(), True),\n",
        "    StructField('fecha_corte', DateType(), True)\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sbPm38eqFkl",
        "outputId": "35eeb48a-5e0c-4a0b-eb12-47104c3e0f60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['27549',\n",
              "  2022,\n",
              "  7,\n",
              "  '16918',\n",
              "  'INSTITUTO NACIONAL DE SALUD DEL NI�O - SAN BORJA',\n",
              "  'LIMA',\n",
              "  'Lima',\n",
              "  'Lima',\n",
              "  'Lima',\n",
              "  '150101.0',\n",
              "  'Z948',\n",
              "  'ESTADO DE TRASPLANTE DE M�DULA �SEA',\n",
              "  'TRASPLANTE DE PROGENITORES HEMATOPOY�TICOS',\n",
              "  'PAC',\n",
              "  'MASCULINO',\n",
              "  5,\n",
              "  'SIS GRATUITO',\n",
              "  'AMBULATORIO',\n",
              "  datetime.date(2022, 6, 27),\n",
              "  0.22,\n",
              "  datetime.date(2023, 5, 25)],\n",
              " ['31973',\n",
              "  2022,\n",
              "  6,\n",
              "  '1703',\n",
              "  'ELEAZAR GUZMAN BARRON',\n",
              "  'ANCASH',\n",
              "  'Ancash',\n",
              "  'Santa',\n",
              "  'Nuevo Chimbote',\n",
              "  '21809.0',\n",
              "  'E119',\n",
              "  'Diabetes mellitus no insulinodependiente, sin menci�n de complicaci�n (MODY)',\n",
              "  'ENFERMEDAD RARA O HUERFANA',\n",
              "  'ERH',\n",
              "  'MASCULINO',\n",
              "  68,\n",
              "  'SIS GRATUITO',\n",
              "  'AMBULATORIO',\n",
              "  datetime.date(2022, 4, 25),\n",
              "  13.46,\n",
              "  datetime.date(2023, 5, 25)],\n",
              " ['8629',\n",
              "  2022,\n",
              "  3,\n",
              "  '6216',\n",
              "  'INSTITUTO NACIONAL DE SALUD DEL NI�O - BRE�A',\n",
              "  'LIMA',\n",
              "  'Lima',\n",
              "  'Lima',\n",
              "  'Bre�a',\n",
              "  '150105.0',\n",
              "  'Q750',\n",
              "  'Craneosinostosis',\n",
              "  'ENFERMEDAD RARA O HUERFANA',\n",
              "  'ERH',\n",
              "  'MASCULINO',\n",
              "  0,\n",
              "  'SIS GRATUITO',\n",
              "  'AMBULATORIO',\n",
              "  datetime.date(2022, 1, 26),\n",
              "  10.69,\n",
              "  datetime.date(2023, 5, 25)]]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\n",
        "# Para formatear las fechas\n",
        "from datetime import datetime\n",
        "def parse_date(date_string):\n",
        "    if date_string:\n",
        "        return datetime.strptime(date_string, '%Y%m%d').date()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "filas = data.filter(lambda x: x != encabezado)\n",
        "\n",
        "# Transformamos el RDD, aplicando el casteo de tipos a las columnas\n",
        "filas = filas.map(lambda x: [\n",
        "    x[0],\n",
        "    int(x[1]),\n",
        "    int(x[2]),\n",
        "    x[3],\n",
        "    x[4],\n",
        "    x[5],\n",
        "    x[6],\n",
        "    x[7],\n",
        "    x[8],\n",
        "    x[9],\n",
        "    x[10],\n",
        "    x[11],\n",
        "    x[12],\n",
        "    x[13],\n",
        "    x[14],\n",
        "    int(x[15]),\n",
        "    x[16],\n",
        "    x[17],\n",
        "    parse_date(x[18]),\n",
        "    float(x[19]),\n",
        "    parse_date(x[20])\n",
        "])\n",
        "\n",
        "filas.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "5weW_mZLqFkl"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = spark.createDataFrame(filas, schema=estructura)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLzgx0p1qFkl",
        "outputId": "c7f68c68-20a5-4a98-bab4-d8338f3dfbf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------+---+------+--------------------+------+------------+---------+--------------+--------+------------------+--------------------+--------------------+---------------+---------+----+------------+-----------+--------------+-----------+-----------+\n",
            "|documento_anonimizado|periodo|mes|renaes|              ipress|region|departamento|provincia|      distrito|  ubigeo|codigo_diagnostico|        diagnosticos|  grupo_diagnosticos|grupo_cobertura|     sexo|edad| tipo_seguro|   servicio|fecha_atencion|monto_bruto|fecha_corte|\n",
            "+---------------------+-------+---+------+--------------------+------+------------+---------+--------------+--------+------------------+--------------------+--------------------+---------------+---------+----+------------+-----------+--------------+-----------+-----------+\n",
            "|                27549|   2022|  7| 16918|INSTITUTO NACIONA...|  LIMA|        Lima|     Lima|          Lima|150101.0|              Z948|ESTADO DE TRASPLA...|TRASPLANTE DE PRO...|            PAC|MASCULINO|   5|SIS GRATUITO|AMBULATORIO|    2022-06-27|       0.22| 2023-05-25|\n",
            "|                31973|   2022|  6|  1703|ELEAZAR GUZMAN BA...|ANCASH|      Ancash|    Santa|Nuevo Chimbote| 21809.0|              E119|Diabetes mellitus...|ENFERMEDAD RARA O...|            ERH|MASCULINO|  68|SIS GRATUITO|AMBULATORIO|    2022-04-25|      13.46| 2023-05-25|\n",
            "|                 8629|   2022|  3|  6216|INSTITUTO NACIONA...|  LIMA|        Lima|     Lima|         Bre�a|150105.0|              Q750|    Craneosinostosis|ENFERMEDAD RARA O...|            ERH|MASCULINO|   0|SIS GRATUITO|AMBULATORIO|    2022-01-26|      10.69| 2023-05-25|\n",
            "+---------------------+-------+---+------+--------------------+------+------------+---------+--------------+--------+------------------+--------------------+--------------------+---------------+---------+----+------------+-----------+--------------+-----------+-----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df.show(3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkXYVcJbqFkm",
        "outputId": "3db6ba58-55fd-4204-a306-8df3d98653fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- documento_anonimizado: string (nullable = true)\n",
            " |-- periodo: integer (nullable = true)\n",
            " |-- mes: integer (nullable = true)\n",
            " |-- renaes: string (nullable = true)\n",
            " |-- ipress: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- departamento: string (nullable = true)\n",
            " |-- provincia: string (nullable = true)\n",
            " |-- distrito: string (nullable = true)\n",
            " |-- ubigeo: string (nullable = true)\n",
            " |-- codigo_diagnostico: string (nullable = true)\n",
            " |-- diagnosticos: string (nullable = true)\n",
            " |-- grupo_diagnosticos: string (nullable = true)\n",
            " |-- grupo_cobertura: string (nullable = true)\n",
            " |-- sexo: string (nullable = true)\n",
            " |-- edad: integer (nullable = true)\n",
            " |-- tipo_seguro: string (nullable = true)\n",
            " |-- servicio: string (nullable = true)\n",
            " |-- fecha_atencion: date (nullable = true)\n",
            " |-- monto_bruto: float (nullable = true)\n",
            " |-- fecha_corte: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTd854ciqFkm",
        "outputId": "bfe2d827-e144-4c5c-cb28-1cb5c10d0605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|             Columna|Porcentaje_Faltantes|\n",
            "+--------------------+--------------------+\n",
            "|documento_anonimi...|                 0.0|\n",
            "|             periodo|                 0.0|\n",
            "|                 mes|                 0.0|\n",
            "|              renaes|                 0.0|\n",
            "|              ipress|  0.2781289506953224|\n",
            "|              region|                 0.0|\n",
            "|        departamento| 0.28396382378683266|\n",
            "|           provincia| 0.28396382378683266|\n",
            "|            distrito| 0.28396382378683266|\n",
            "|              ubigeo| 0.28396382378683266|\n",
            "|  codigo_diagnostico|                 0.0|\n",
            "|        diagnosticos|                 0.0|\n",
            "|  grupo_diagnosticos|                 0.0|\n",
            "|     grupo_cobertura|                 0.0|\n",
            "|                sexo|                 0.0|\n",
            "|                edad|                 0.0|\n",
            "|         tipo_seguro|                 0.0|\n",
            "|            servicio|                 0.0|\n",
            "|      fecha_atencion|                 0.0|\n",
            "|         monto_bruto|                 0.0|\n",
            "|         fecha_corte|                 0.0|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#DEPURACION\n",
        "#1. VERIFICAR EL % DE FILAS CON VALORES NULOS O ESPACIOS VACIOS\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el porcentaje de datos faltantes (nulos o vacíos) para cada columna\n",
        "porcentaje_faltantes = [(col, (df.filter((F.col(col).isNull()) | (F.col(col) == \"\") | (F.trim(F.col(col)) == \"\")).count() / df.count()) * 100) for col in df.columns]\n",
        "\n",
        "# Convertir el resultado a un DataFrame sin redondeo\n",
        "df_porcentaje = spark.createDataFrame(porcentaje_faltantes, [\"Columna\", \"Porcentaje_Faltantes\"])\n",
        "\n",
        "# Mostrar el DataFrame con todos los decimales\n",
        "df_porcentaje.show(21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPa2hqdTqFkm",
        "outputId": "7f423d83-59ed-473e-ba3f-68f38d465cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de filas original: 51415\n",
            "Número de filas después de la limpieza: 51126\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(f\"Número de filas original: {df.count()}\")\n",
        "\n",
        "# Crear una condición combinada para identificar filas con valores nulos o vacíos\n",
        "condicion = None\n",
        "for col in df.columns:\n",
        "    nueva_condicion = (F.col(col).isNull()) | (F.trim(F.col(col)) == \"\")  # Verificar nulos o espacios vacíos\n",
        "    if condicion is None:\n",
        "        condicion = nueva_condicion\n",
        "    else:\n",
        "        condicion = condicion | nueva_condicion\n",
        "\n",
        "# Filtrar las filas que NO cumplen con la condición (es decir, eliminar las filas con nulos o vacíos)\n",
        "df = df.filter(~condicion)\n",
        "\n",
        "# Mostrar el número de filas después de eliminar las filas con nulos o vacíos\n",
        "print(f\"Número de filas después de la limpieza: {df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkWdebh3qFkm",
        "outputId": "7252aa5c-a2c4-40c2-8037-0813b5795861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de filas original sin vacios: 51126\n",
            "Número de filas original con la ENFERMEDAD RARA: 48184\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Número de filas original sin vacios: {df.count()}\")\n",
        "df = df[df['GRUPO_DIAGNOSTICOS'] == 'ENFERMEDAD RARA O HUERFANA']\n",
        "print(f\"Número de filas original con la ENFERMEDAD RARA: {df.count()}\")\n",
        "#print(df['GRUPO_DIAGNOSTICOS'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOicxskPqFkm",
        "outputId": "88d5ff72-fdab-4e25-f98c-f722fb8930f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------+----------------------------+-----------+\n",
            "|Fechas_ATENCION_mal_formateadas|Fechas_CORTE_mal_formateadas|Total_filas|\n",
            "+-------------------------------+----------------------------+-----------+\n",
            "|                              0|                           0|      48184|\n",
            "+-------------------------------+----------------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#1. DEPURACIÓN DE FORMATO Y CONSISTENCIA DE LAS FECHAS\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "# Depuración de formato y consistencia de las fechas\n",
        "df = df.withColumn(\"FECHA_ATENCION_VALID\", to_date(col(\"FECHA_ATENCION\"), \"yyyyMMdd\").isNotNull())\n",
        "df = df.withColumn(\"FECHA_CORTE_VALID\", to_date(col(\"FECHA_CORTE\"), \"yyyyMMdd\").isNotNull())\n",
        "\n",
        "# Creación de una tabla resumen con la cantidad de fechas mal formateadas\n",
        "df_summary = df.agg(\n",
        "    F.sum(F.when(~col(\"FECHA_ATENCION_VALID\"), 1).otherwise(0)).alias(\"Fechas_ATENCION_mal_formateadas\"),\n",
        "    F.sum(F.when(~col(\"FECHA_CORTE_VALID\"), 1).otherwise(0)).alias(\"Fechas_CORTE_mal_formateadas\"),\n",
        "    F.count(\"*\").alias(\"Total_filas\")\n",
        ")\n",
        "\n",
        "df_summary.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx4LkwV8qFkm",
        "outputId": "43a4fdfb-d1fb-4f20-a68a-a653091f898a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de filas original sin vacios y solo con ENFERMEDAD: 48184\n",
            "Número de filas después de la limpieza de edad: 48184\n",
            "Número de filas después de la limpieza del monto: 48184\n",
            "Número de filas después de la limpieza del mes: 48184\n",
            "+---------+\n",
            "|     SEXO|\n",
            "+---------+\n",
            "| FEMENINO|\n",
            "|MASCULINO|\n",
            "+---------+\n",
            "\n",
            "+-----------------+\n",
            "|      TIPO_SEGURO|\n",
            "+-----------------+\n",
            "|  SIS EMPRENDEDOR|\n",
            "|     SIS GRATUITO|\n",
            "|SIS INDEPENDIENTE|\n",
            "|   SIS PARA TODOS|\n",
            "|SIS MICROEMPRESAS|\n",
            "+-----------------+\n",
            "\n",
            "+-------------+\n",
            "|       REGION|\n",
            "+-------------+\n",
            "|     AYACUCHO|\n",
            "|     APURIMAC|\n",
            "|       CALLAO|\n",
            "|  LA LIBERTAD|\n",
            "|     AMAZONAS|\n",
            "|      UCAYALI|\n",
            "|       TUMBES|\n",
            "|        JUNIN|\n",
            "|     AREQUIPA|\n",
            "|         PUNO|\n",
            "|   LAMBAYEQUE|\n",
            "|MADRE DE DIOS|\n",
            "|      HUANUCO|\n",
            "|        PIURA|\n",
            "| HUANCAVELICA|\n",
            "|       LORETO|\n",
            "|    CAJAMARCA|\n",
            "|     MOQUEGUA|\n",
            "|   SAN MARTIN|\n",
            "|        TACNA|\n",
            "|        CUSCO|\n",
            "|          ICA|\n",
            "|       ANCASH|\n",
            "|         LIMA|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Número de filas original sin vacios y solo con ENFERMEDAD: {df.count()}\")\n",
        "\n",
        "#2. DEPURACION Verificar valores inconsistentes de las edades\n",
        "df = df.filter((col(\"EDAD\") >= 0) & (col(\"EDAD\") <= 110))\n",
        "print(f\"Número de filas después de la limpieza de edad: {df.count()}\")\n",
        "\n",
        "#3. Verificar que MONTO_BRUTO sea un valor positivo\n",
        "df=df.filter(col(\"MONTO_BRUTO\") >= 0)\n",
        "print(f\"Número de filas después de la limpieza del monto: {df.count()}\")\n",
        "\n",
        "#4. DEPURACION Verificar valores inconsistentes de los meses\n",
        "df=df.filter((col(\"MES\") > 0) | (col(\"MES\") <= 12))\n",
        "print(f\"Número de filas después de la limpieza del mes: {df.count()}\")\n",
        "\n",
        "#4. Verificar valores inconsistentes en la columna SEXO\n",
        "df.select(\"SEXO\").distinct().show()\n",
        "\n",
        "#5. Verificar valores inconsistentes en la columna TIPO_SEGURO\n",
        "df.select(\"TIPO_SEGURO\").distinct().show()\n",
        "\n",
        "#5. Verificar valores inconsistentes en la columna TIPO_SEGURO\n",
        "df.select(\"REGION\").distinct().show(24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG2iSl4oqFkm",
        "outputId": "052ca371-71ed-4ccd-e58c-a507289fc8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas antes de eliminar duplicados: 48184\n",
            "Filas después de eliminar duplicados: 47645\n",
            "Cantidad de duplicados eliminados: 539\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#6. Eliminar filas duplicadas, si es que lo hay\n",
        "filas_antes = df.count()\n",
        "df = df.dropDuplicates()\n",
        "filas_despues = df.count()\n",
        "\n",
        "print(f\"Filas antes de eliminar duplicados: {filas_antes}\")\n",
        "print(f\"Filas después de eliminar duplicados: {filas_despues}\")\n",
        "print(f\"Cantidad de duplicados eliminados: {filas_antes - filas_despues}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "UA96qnu0qFkn",
        "outputId": "f3373d8b-2263-4174-a4aa-7aa35e3aa9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-ca5031936a6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calcular el porcentaje de datos faltantes (nulos o vacíos) para cada columna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mporcentaje_faltantes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Convertir el resultado a un DataFrame sin redondeo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-ca5031936a6f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calcular el porcentaje de datos faltantes (nulos o vacíos) para cada columna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mporcentaje_faltantes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Convertir el resultado a un DataFrame sin redondeo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \"\"\"\n\u001b[0;32m-> 1238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "#Código de verificación\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el porcentaje de datos faltantes (nulos o vacíos) para cada columna\n",
        "porcentaje_faltantes = [(col, (df.filter((F.col(col).isNull()) | (F.col(col) == \"\") | (F.trim(F.col(col)) == \"\")).count() / df.count()) * 100) for col in df.columns]\n",
        "\n",
        "# Convertir el resultado a un DataFrame sin redondeo\n",
        "df_porcentaje = spark.createDataFrame(porcentaje_faltantes, [\"Columna\", \"Porcentaje_Faltantes\"])\n",
        "\n",
        "# Mostrar el DataFrame con todos los decimales\n",
        "df_porcentaje.show(21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "AL409aGLqFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# INDICADOR 1\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import sum as _sum, count, round, col\n",
        "\n",
        "# INDICADOR 1: Agrupación por GRUPO_COBERTURA, SEXO y DIAGNOSTICOS con orden alfabético\n",
        "resultado1 = df.groupBy(\"SEXO\", \"DIAGNOSTICOS\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"TOTAL_ATENCIONES\"),\n",
        "        round((_sum(\"MONTO_BRUTO\") / count(\"*\")), 2).alias(\"COSTO_PROMEDIO_ATENCION\")\n",
        "    ) \\\n",
        "    .orderBy(\"DIAGNOSTICOS\")\n",
        "\n",
        "print(\"COSTO PROMEDIO Y TOTAL DE ATENCIONES POR SEXO:\")\n",
        "resultado1.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "gmKftC82qFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#INDICADOR 2\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el número de diagnósticos por sexo\n",
        "diagnosticos_por_sexo = df.groupBy(\"SEXO\").agg(\n",
        "    F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_DIAGNOSTICOS\")\n",
        ")\n",
        "\n",
        "# Calcular el total general de diagnósticos\n",
        "total_diagnosticos = df.select(F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_GENERAL\")).collect()[0][\"TOTAL_GENERAL\"]\n",
        "\n",
        "# Calcular el porcentaje de diagnósticos por sexo\n",
        "porcentaje_diagnosticos = diagnosticos_por_sexo.withColumn(\n",
        "    \"PORCENTAJE_DIAGNOSTICOS(%)\",\n",
        "    F.round((F.col(\"TOTAL_DIAGNOSTICOS\") / total_diagnosticos) * 100, 2)\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por SEXO\n",
        "porcentaje_diagnosticos.orderBy(\"SEXO\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "5t9J68i0qFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#INDICADOR 3\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, sum as _sum, count, round\n",
        "\n",
        "# Calcular el costo promedio y el total de atenciones agrupando por tipo de seguro y diagnóstico\n",
        "resultado3 = df.groupBy(\"TIPO_SEGURO\", \"DIAGNOSTICOS\").agg(\n",
        "    count(\"*\").alias(\"TOTAL_ATENCIONES\"),  # Total de atenciones\n",
        "    round(_sum(\"MONTO_BRUTO\") / count(\"*\"), 2).alias(\"COSTO_PROMEDIO\")  # Costo promedio usando _sum\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por tipo de seguro\n",
        "resultado3.orderBy(\"TIPO_SEGURO\").show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "E4cwf_xZqFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#INDICADOR 4\n",
        "# Contamos el número total de atenciones en todas las regiones\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "total_atenciones = df.count()\n",
        "\n",
        "# Contamos el número de atenciones por cada región\n",
        "atenciones_por_region = df.groupBy(\"REGION\").count().withColumnRenamed(\"count\", \"TOTAL_ATENCIONES\")\n",
        "\n",
        "# Calculamos la tasa de atención por región y redondeamos en una sola operación\n",
        "tasa_atencion_region = atenciones_por_region.withColumn(\n",
        "    \"Tasa de Atención (%)\",\n",
        "    F.round((col(\"TOTAL_ATENCIONES\") / total_atenciones) * 100, 2)\n",
        ")\n",
        "\n",
        "# Mostramos el resultado\n",
        "tasa_atencion_region.show(24)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "HJ6jjnXQqFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#ESTADISTICA DESCRIPTIVA\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "# Calcular las estadísticas numéricas\n",
        "numerical_stats = df.select(\"EDAD\", \"MONTO_BRUTO\") \\\n",
        "    .summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\")\n",
        "\n",
        "# Redondear a 2 decimales\n",
        "numerical_stats_rounded = numerical_stats.select(\n",
        "    \"summary\",\n",
        "    round(\"EDAD\", 2).alias(\"EDAD\"),\n",
        "    round(\"MONTO_BRUTO\", 2).alias(\"MONTO_BRUTO\")\n",
        ")\n",
        "\n",
        "# Mostrar el resultado\n",
        "numerical_stats_rounded.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "4kapmirmqFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#CORRELACION ENTRE DATOS\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "# Preparar datos para correlación\n",
        "df = df.withColumn(\"EDAD\", col(\"EDAD\").cast(\"integer\"))\n",
        "df = df.withColumn(\"MONTO_BRUTO\", col(\"MONTO_BRUTO\").cast(\"float\"))\n",
        "\n",
        "vector_cols = [\"EDAD\", \"MONTO_BRUTO\"]\n",
        "assembler = VectorAssembler(inputCols=vector_cols, outputCol=\"features\")\n",
        "df_vector = assembler.transform(df)\n",
        "\n",
        "\n",
        "# Calcular matriz de correlación\n",
        "matrix = Correlation.corr(df_vector, \"features\").head()[0]\n",
        "correlation_matrix = matrix.toArray().tolist()\n",
        "\n",
        "for row in correlation_matrix:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "OEgzj6ysqFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#ANALISIS ESTADISTICO\n",
        "# KPI 1: Análisis detallado\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, count, round, avg, stddev, sum as _sum, when\n",
        "\n",
        "# Calcular métricas detalladas\n",
        "resultado1_detallado = df.groupBy(\"SEXO\", \"DIAGNOSTICOS\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"TOTAL_ATENCIONES\"),  # Total de atenciones\n",
        "        round(avg(\"MONTO_BRUTO\"), 2).alias(\"COSTO_PROMEDIO_ATENCION\"),  # Promedio de MONTO_BRUTO\n",
        "        # Si el total de atenciones es 1, la desviación estándar será 0\n",
        "        round(when(count(\"*\") == 1, 0).otherwise(stddev(\"MONTO_BRUTO\")), 2).alias(\"DESVIACION_ESTANDAR_MONTO\"),\n",
        "        round(avg(\"EDAD\"), 2).alias(\"PROMEDIO_EDAD\")  # Promedio de la edad\n",
        "    ) \\\n",
        "    .orderBy(\"DIAGNOSTICOS\")\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"ANÁLISIS DETALLADO: COSTO PROMEDIO, DESVIACIÓN ESTÁNDAR Y PROMEDIO DE EDAD POR SEXO Y DIAGNÓSTICO:\")\n",
        "resultado1_detallado.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "gZFMIkL8qFkn"
      },
      "outputs": [],
      "source": [
        "\n",
        "#INDICADOR 2: ANALISIS DETALLADO\n",
        "#INDICADOR 2: SEXO, TOTAL DE DIAGNOSTICOS, PORCENTAJE, EDAD(PROMEDIO), MONTO_BRUTO(PROMEDIO Y DES.EST)\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el número de diagnósticos, edad promedio, monto promedio y desviación estándar por sexo\n",
        "diagnosticos_detallado = df.groupBy(\"SEXO\").agg(\n",
        "    F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_DIAGNOSTICOS\"),               # Total de diagnósticos\n",
        "    F.round(F.avg(\"EDAD\"), 2).alias(\"EDAD_PROMEDIO\"),                  # Promedio de edad\n",
        "    F.round(F.avg(\"MONTO_BRUTO\"), 2).alias(\"PROMEDIO_MONTO_BRUTO\"),    # Promedio del monto bruto\n",
        "    F.round(F.stddev(\"MONTO_BRUTO\"), 2).alias(\"DESVIACION_MONTO_BRUTO\")# Desviación estándar del monto bruto\n",
        ")\n",
        "\n",
        "# Calcular el total general de diagnósticos\n",
        "total_diagnosticos = df.select(F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_GENERAL\")).collect()[0][\"TOTAL_GENERAL\"]\n",
        "\n",
        "# Calcular el porcentaje de diagnósticos por sexo\n",
        "diagnosticos_con_porcentaje = diagnosticos_detallado.withColumn(\n",
        "    \"PORCENTAJE_DIAGNOSTICOS(%)\",\n",
        "    F.round((F.col(\"TOTAL_DIAGNOSTICOS\") / total_diagnosticos) * 100, 2)\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por SEXO\n",
        "diagnosticos_con_porcentaje.orderBy(\"SEXO\").show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "zg1BabEvqFko"
      },
      "outputs": [],
      "source": [
        "\n",
        "#INDICADOR 3: ANALISIS DETALLADO\n",
        "#INDICADOR 3: TIPO DE SEGURO, DIAGNOSTICO, MONTO(PROMEDIO Y DESV.), TOTAL DE ATENCIONES, EDAD(PROMEDIO)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, sum as _sum, count, round, avg, stddev, when\n",
        "\n",
        "# Calcular el costo promedio, la desviación estándar y el promedio de edad agrupando por tipo de seguro y diagnóstico\n",
        "resultado_detallado = df.groupBy(\"TIPO_SEGURO\", \"DIAGNOSTICOS\").agg(\n",
        "    count(\"*\").alias(\"TOTAL_ATENCIONES\"),  # Total de atenciones\n",
        "    round(avg(\"MONTO_BRUTO\"), 2).alias(\"PROMEDIO_MONTO_BRUTO\"),  # Promedio del MONTO_BRUTO\n",
        "    # Si el total de atenciones es 1, la desviación estándar será 0\n",
        "    round(when(count(\"*\") == 1, 0).otherwise(stddev(\"MONTO_BRUTO\")), 2).alias(\"DESVIACION_MONTO_BRUTO\"),\n",
        "    round(avg(\"EDAD\"), 2).alias(\"PROMEDIO_EDAD\")  # Promedio de la edad\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por tipo de seguro y diagnóstico\n",
        "resultado_detallado.orderBy(\"TIPO_SEGURO\", \"DIAGNOSTICOS\").show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "XEWQLRWGqFko"
      },
      "outputs": [],
      "source": [
        "\n",
        "#INDICADOR 4: ANALISIS DETALLADO\n",
        "#INDICADOR 4: REGION, TOTAL DE ATENCIONES, PORCENTAJE, EDAD(PROMEDIO), MONTO_BRUTO(PROMEDIO Y DES. ESTA)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, avg, stddev, round, count, when\n",
        "\n",
        "# Contar el número total de atenciones en todas las regiones\n",
        "total_atenciones = df.count()\n",
        "\n",
        "# Agrupamos por región, calculamos las métricas y ajustamos la desviación estándar si hay solo una atención\n",
        "analisis_region = df.groupBy(\"REGION\").agg(\n",
        "    count(\"*\").alias(\"TOTAL_ATENCIONES\"),\n",
        "    round(avg(\"EDAD\"), 2).alias(\"PROMEDIO_EDAD\"),\n",
        "    round(avg(\"MONTO_BRUTO\"), 2).alias(\"PROMEDIO_MONTO_BRUTO\"),\n",
        "    round(when(count(\"*\") == 1, 0).otherwise(stddev(\"MONTO_BRUTO\")), 2).alias(\"DESVIACION_MONTO_BRUTO\")\n",
        ").withColumn(\n",
        "    \"PORCENTAJE_ATENCIONES(%)\",\n",
        "    round((col(\"TOTAL_ATENCIONES\") / total_atenciones) * 100, 2)\n",
        ").orderBy(col(\"PORCENTAJE_ATENCIONES(%)\").desc())\n",
        "\n",
        "# Mostrar el resultado\n",
        "analisis_region.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "e-qQ_W-eqFko"
      },
      "outputs": [],
      "source": [
        "\n",
        "#GRAFICOS\n",
        "#INDICADOR 1\n",
        "import matplotlib.pyplot as plt\n",
        "# Convertir el DataFrame de Spark en un DataFrame de pandas\n",
        "data_kpi = resultado1.orderBy(\"COSTO_PROMEDIO_ATENCION\", ascending=False).toPandas()\n",
        "\n",
        "# Seleccionar los 20 diagnósticos con mayor costo promedio\n",
        "top_data = data_kpi.head(20)\n",
        "\n",
        "# Crear el gráfico de barras horizontales, diferenciando por SEXO\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Colorear las barras según el SEXO (azul para masculino, rosa para femenino)\n",
        "for sexo in top_data['SEXO'].unique():\n",
        "    subset = top_data[top_data['SEXO'] == sexo]\n",
        "    plt.barh(subset[\"DIAGNOSTICOS\"], subset[\"COSTO_PROMEDIO_ATENCION\"], label=sexo, alpha=0.7)\n",
        "\n",
        "# Añadir etiquetas y título\n",
        "plt.xlabel(\"Costo Promedio de Atención\")\n",
        "plt.ylabel(\"Diagnósticos\")\n",
        "plt.title(\"Top 20 Diagnósticos con Mayor Costo Promedio de Atención por Sexo\")\n",
        "\n",
        "# Invertir el eje Y para que el diagnóstico con mayor costo esté arriba\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Añadir una leyenda para SEXO\n",
        "plt.legend(title=\"Sexo\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "wcb13XNMqFko"
      },
      "outputs": [],
      "source": [
        "\n",
        "# INDICADOR 2:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "data_sexo = porcentaje_diagnosticos.orderBy(\"SEXO\").toPandas()\n",
        "\n",
        "# Crear el gráfico de barras\n",
        "plt.figure(figsize=(10, 6))  # Ajusté el tamaño para mejor visualización\n",
        "plt.bar(data_sexo[\"SEXO\"], data_sexo[\"PORCENTAJE_DIAGNOSTICOS(%)\"], color=\"salmon\")\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.xlabel(\"Sexo\")\n",
        "plt.ylabel(\"Porcentaje de Diagnósticos (%)\")\n",
        "plt.title(\"Porcentaje de Diagnósticos por Sexo\")\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.xticks(rotation=0)  # No es necesario rotar ya que solo son dos categorías\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "FJ32C_OhqFko"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convertir el DataFrame de Spark en un DataFrame de pandas\n",
        "data_kpi3_1 = resultado3.orderBy(\"COSTO_PROMEDIO\", ascending=False).toPandas()\n",
        "\n",
        "# Seleccionar los 20 diagnósticos con mayor costo promedio\n",
        "top_data3_1 = data_kpi3_1.head(20)\n",
        "\n",
        "# Crear el gráfico de barras agrupadas por TIPO_SEGURO para cada DIAGNOSTICO\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Crear barras agrupadas por TIPO_SEGURO para cada DIAGNOSTICO\n",
        "for tipo_seguro in top_data3_1[\"TIPO_SEGURO\"].unique():\n",
        "    subset = top_data3_1[top_data3_1[\"TIPO_SEGURO\"] == tipo_seguro]\n",
        "    plt.barh(subset[\"DIAGNOSTICOS\"], subset[\"COSTO_PROMEDIO\"], label=tipo_seguro, alpha=0.7)\n",
        "\n",
        "# Añadir etiquetas y título\n",
        "plt.xlabel(\"Costo Promedio\")\n",
        "plt.ylabel(\"Diagnósticos\")\n",
        "plt.title(\"Top 20 Diagnósticos con Mayor Costo Promedio por Tipo de Seguro\")\n",
        "\n",
        "# Invertir el eje Y para que el diagnóstico con mayor costo esté arriba\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Añadir una leyenda para los tipos de seguro\n",
        "plt.legend(title=\"Tipo de Seguro\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ef5MH60sqFko"
      },
      "outputs": [],
      "source": [
        "\n",
        "# INDICADOR 4:\n",
        "# Mostrar tasa de atención por \"REGION\"\n",
        "data_region = tasa_atencion_region.orderBy(\"Tasa de Atención (%)\").toPandas()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(data_region[\"REGION\"], data_region[\"Tasa de Atención (%)\"], color=\"violet\")\n",
        "plt.xlabel(\"Región\")\n",
        "plt.ylabel(\"Tasa de Atención (%)\")\n",
        "plt.title(\"Tasa de Atención por Región\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "wa2HURfdqFko"
      },
      "outputs": [],
      "source": [
        "\n",
        "#METRICAS DE RENDIMIENTO\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, count, round, avg, stddev\n",
        "\n",
        "# Calcula las métricas de rendimiento global\n",
        "performance_metrics = df.agg(\n",
        "    countDistinct(\"DIAGNOSTICOS\").alias(\"Total Diagnósticos\"),  # Total de diagnósticos únicos\n",
        "    round(avg(\"MONTO_BRUTO\"), 2).alias(\"Promedio Monto Bruto\"),  # Promedio del monto bruto\n",
        "    round(stddev(\"MONTO_BRUTO\"), 2).alias(\"Desviación Estándar Monto\"),  # Desviación estándar del monto bruto\n",
        "    round(avg(\"EDAD\"), 2).alias(\"Promedio Edad\"),  # Promedio de la edad\n",
        "    count(\"*\").alias(\"Total Atenciones\") #Total de atenciones\n",
        ").collect()[0].asDict()\n",
        "\n",
        "print(\"\\n=== Métricas de Rendimiento Global ===\")\n",
        "for field, value in performance_metrics.items(): #Iterar sobre el diccionario\n",
        "    print(f\"{field}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "OrR22ZuwqFkp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#MODELO PREDICTIVO\n",
        "# 1. Distribución de la variable objetivo\n",
        "if 'MONTO_BRUTO' in df.columns:\n",
        "    df.groupBy('MONTO_BRUTO').count().show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "oH7jHLaOqFkp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Lista de columnas a eliminar\n",
        "columns_to_drop = ['DOCUMENTO_ANONIMIZADO', 'RENAES', 'DISTRITO', 'UBIGEO', 'SERVICIO', 'CODIGO_DIAGNOSTICO']\n",
        "\n",
        "# Eliminar las columnas del DataFrame\n",
        "df = df.drop(*columns_to_drop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4UUVFVxqFkp",
        "outputId": "f3f36fda-a33f-483e-87a4-b949eef383ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+------------------+--------------------+--------+------------+---------+--------------------+--------------------+---------------+---------+------------------+---------------+-----------------+\n",
            "|summary|             periodo|               mes|              ipress|  region|departamento|provincia|        diagnosticos|  grupo_diagnosticos|grupo_cobertura|     sexo|              edad|    tipo_seguro|      monto_bruto|\n",
            "+-------+--------------------+------------------+--------------------+--------+------------+---------+--------------------+--------------------+---------------+---------+------------------+---------------+-----------------+\n",
            "|  count|               47645|             47645|               47645|   47645|       47645|    47645|               47645|               47645|          47645|    47645|             47645|          47645|            47645|\n",
            "|   mean|              2022.0| 4.233518732290902|                NULL|    NULL|        NULL|     NULL|                NULL|                NULL|           NULL|     NULL| 41.62571098751181|           NULL|77.31900234378642|\n",
            "| stddev|1.136853531711088...|1.9648273094561324|                NULL|    NULL|        NULL|     NULL|                NULL|                NULL|           NULL|     NULL|25.601114386810124|           NULL|5330.044660488442|\n",
            "|    min|                2022|                 1|\"HOSPITAL CHANCAY...|AMAZONAS|    Amazonas|  Abancay|      Acantamebiasis|ENFERMEDAD RARA O...|            ERH| FEMENINO|                 0|SIS EMPRENDEDOR|              0.0|\n",
            "|    max|                2022|                 7|SANTA MARIA DEL S...| UCAYALI|     Ucayali|   Tumbes|Xeroderma pigmentoso|ENFERMEDAD RARA O...|            ERH|MASCULINO|               105| SIS PARA TODOS|        1095328.0|\n",
            "+-------+--------------------+------------------+--------------------+--------+------------+---------+--------------------+--------------------+---------------+---------+------------------+---------------+-----------------+\n",
            "\n",
            "Matriz de correlaciones (ordenada por columnas numércias):\n",
            "DenseMatrix([[1.        ,        nan,        nan,        nan],\n",
            "             [       nan, 1.        , 0.0369616 , 0.00612095],\n",
            "             [       nan, 0.0369616 , 1.        , 0.00113338],\n",
            "             [       nan, 0.00612095, 0.00113338, 1.        ]])\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.sql.functions import count, when, isnan, col\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "#2. Análisis estadístico de columnas numéricas\n",
        "df.describe().show()\n",
        "\n",
        "# 4. Verificar correlaciones entre columnas numéricas\n",
        "numeric_columns= [col_name for col_name, dtype in df.dtypes if dtype in ['int', 'double', 'float']]\n",
        "if numeric_columns:\n",
        "    assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"features\")\n",
        "    df_assembled = assembler.transform(df)\n",
        "    correlation_matrix = Correlation.corr(df_assembled, \"features\").head()[0]\n",
        "    print(f\"Matriz de correlaciones (ordenada por columnas numércias):\")\n",
        "    print (correlation_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "wSjwWWkkqFkp",
        "outputId": "8094c3af-7d12-4671-8757-c4c4eed82e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|      (7,[6],[75.0])| 1.26|\n",
            "|[0.0,0.0,8.0,8.0,...|  4.8|\n",
            "|(7,[2,3,4],[16.0,...| 41.6|\n",
            "|(7,[0,6],[1.0,61.0])| 16.2|\n",
            "|(7,[0,6],[1.0,14.0])|10.69|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 4 has 39 values. Consider removing this and other categorical features with a large number of values, or add more training examples.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-9c20ee81bc3d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mdt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mgbt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Hacer predicciones en los datos de prueba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: DecisionTree requires maxBins (= 32) to be at least as large as the number of values in each categorical feature, but categorical feature 4 has 39 values. Consider removing this and other categorical features with a large number of values, or add more training examples."
          ]
        }
      ],
      "source": [
        "# Lista de columnas a eliminar\n",
        "columns_to_drop = ['DOCUMENTO_ANONIMIZADO', 'PERIODO', 'RENAES', 'DISTRITO', 'UBIGEO', 'SERVICIO', 'CODIGO_DIAGNOSTICO', 'ipress']\n",
        "\n",
        "# Eliminar las columnas del DataFrame\n",
        "df = df.drop(*columns_to_drop)\n",
        "\n",
        "# 13. Preparación de los datos para el modelo de regresión\n",
        "# Definir las columnas categóricas y numéricas\n",
        "categorical_cols = ['sexo', 'tipo_seguro','region','departamento','provincia','grupo_diagnosticos'] #se saca ipress de las columnas categoricas\n",
        "numerical_cols = ['edad']\n",
        "\n",
        "# Crear StringIndexers para las columnas categóricas\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "\n",
        "# Crear VectorAssembler para combinar las características\n",
        "assembler_inputs = [col + \"_index\" for col in categorical_cols] + numerical_cols\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "# Crear un Pipeline para transformar los datos\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "\n",
        "# Ajustar el Pipeline a los datos y transformar los datos\n",
        "pipeline_model = pipeline.fit(df)\n",
        "df_transformed = pipeline_model.transform(df)\n",
        "\n",
        "# Seleccionar las columnas relevantes para el modelo\n",
        "model_df = df_transformed.select(\"features\", col(\"MONTO_BRUTO\").alias(\"label\"))  # \"label\" es el nombre estándar para la variable objetivo\n",
        "\n",
        "# Mostrar los datos transformados\n",
        "model_df.show(5)\n",
        "\n",
        "# 14. Entrenar y evaluar múltiples modelos de regresión\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_data, test_data = model_df.randomSplit([0.7, 0.3], seed=1234)\n",
        "\n",
        "# Definir los modelos de regresión\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\", maxBins=72)  # Aumentar maxBins\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxBins=72)  # Aumentar maxBins\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\", maxIter=100) # GBT does not have maxBins\n",
        "\n",
        "# Entrenar los modelos\n",
        "lr_model = lr.fit(train_data)\n",
        "dt_model = dt.fit(train_data)\n",
        "rf_model = rf.fit(train_data)\n",
        "gbt_model = gbt.fit(train_data)\n",
        "\n",
        "# Hacer predicciones en los datos de prueba\n",
        "lr_predictions = lr_model.transform(test_data)\n",
        "dt_predictions = dt_model.transform(test_data)\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "gbt_predictions = gbt_model.transform(test_data)\n",
        "\n",
        "# 15. Evaluar los modelos utilizando RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")  # RMSE es una métrica común para regresión\n",
        "\n",
        "# Calcular RMSE para cada modelo\n",
        "lr_rmse = evaluator.evaluate(lr_predictions)\n",
        "dt_rmse = evaluator.evaluate(dt_predictions)\n",
        "rf_rmse = evaluator.evaluate(rf_predictions)\n",
        "gbt_rmse = evaluator.evaluate(gbt_predictions)\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(\"Resultados de la evaluación:\")\n",
        "print(f\"Linear Regression RMSE: {lr_rmse}\")\n",
        "print(f\"Decision Tree RMSE: {dt_rmse}\")\n",
        "print(f\"Random Forest RMSE: {rf_rmse}\")\n",
        "print(f\"GBT RMSE: {gbt_rmse}\")\n",
        "\n",
        "# Calcular R-squared para cada modelo\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
        "dt_r2 = evaluator_r2.evaluate(dt_predictions)\n",
        "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
        "gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
        "print(f\"Linear Regression R-squared: {lr_r2}\")\n",
        "print(f\"Decision Tree R-squared: {dt_r2}\")\n",
        "print(f\"Random Forest R-squared: {rf_r2}\")\n",
        "print(f\"GBT R-squared: {gbt_r2}\")\n",
        "\n",
        "# 16. Seleccionar el mejor modelo y realizar predicciones\n",
        "# Basado en los resultados, selecciona el modelo con el RMSE más bajo (o la métrica que consideres más importante).\n",
        "# En este ejemplo, asumiremos que el Random Forest Regression es el mejor modelo.\n",
        "# Puedes usar el modelo seleccionado para hacer predicciones sobre nuevos datos.\n",
        "# Para hacer esto, debes transformar los nuevos datos usando el mismo Pipeline que usaste para entrenar el modelo.\n",
        "# Ejemplo de cómo hacer predicciones con el mejor modelo (Random Forest):\n",
        "best_model = rf_model  # Asumiendo que Random Forest es el mejor modelo\n",
        "\n",
        "# Puedes guardar el mejor modelo para usarlo en el futuro\n",
        "\n",
        "# best_model.save(\"ruta/al/modelo\")\n",
        "\n",
        "print(\"¡Modelo entrenado y evaluado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzJ4ka2fqFkp",
        "outputId": "b24a931d-0781-4294-fb2c-fc87c9723fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columna categórica ipress' tiene 71 valores únicos.\n",
            "Columna categórica region' tiene 24 valores únicos.\n",
            "Columna categórica departamento' tiene 24 valores únicos.\n",
            "Columna categórica provincia' tiene 38 valores únicos.\n",
            "Columna categórica diagnosticos' tiene 430 valores únicos.\n",
            "Columna categórica grupo_diagnosticos' tiene 1 valores únicos.\n",
            "Columna categórica grupo_cobertura' tiene 1 valores únicos.\n",
            "Columna categórica sexo' tiene 2 valores únicos.\n",
            "Columna categórica tipo_seguro' tiene 5 valores únicos.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#4. Evaluación de categorías en columnas categóricas\n",
        "categorical_columns = [col_name for col_name, dtype in df.dtypes if dtype == 'string']\n",
        "for col_name in categorical_columns:\n",
        "    unique_count = df.select(col_name).distinct().count()\n",
        "    print(f\"Columna categórica {col_name}' tiene {unique_count} valores únicos.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8slZDgfjqFkp",
        "outputId": "5a947dd3-a2b6-441a-b2f0-55384e8eac2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sugerencia: Modelos de regresión como Linear Regression podrían ser útiles.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#5. Recomendación preliminar del modelo basado en análisis de datos\n",
        "if len(numeric_columns) > 3 and df.select('MONTO_BRUTO').distinct().count() == 2:\n",
        "    print(\"Sugerencia: Modelos de clasificación binaria como Logistic Regression o Random Forest son adecuados.\")\n",
        "elif len(numeric_columns) > 3:\n",
        "  print(\"Sugerencia: Modelos de regresión como Linear Regression podrían ser útiles.\")\n",
        "else:\n",
        "  print(\"Sugerencia: Realiza más análisis para identificar relaciones complejas y considerar modelos no lineales.\")\n",
        "\n",
        "#ESTOS CODIGOS TENEMOS QUE HACER PARA VER QUE RECOMIENDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "LQtidrzOqFkp"
      },
      "outputs": [],
      "source": [
        "#6. ESTE CODIGO YA DEFINE QUE MODELO UTILIZAR\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit']\n",
        "df.show(5)\n",
        "from pyspark.ml import Pipeline\n",
        "# Define los indexers\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_columns]\n",
        "# Configura el pipeline\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "# Ajusta y transforma los datos\n",
        "df_transformed = pipeline.fit(df).transform(df)\n",
        "df_transformed.show(5)\n",
        "feature_columns = [col + \"_index\" for col in categorical_columns[:-1]] + ['balance', 'duration', 'campaign', 'pdays', 'previous']\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "df_transformed = assembler.transform(df_transformed)\n",
        "df_transformed = df_transformed.select(\"features\", \"deposit_index\") # Selecciona las columnas necesarias\n",
        "df_transformed.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "P2DrnkDyqFkp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#7. PERO POR LO MENOS ESTO SI TENEMOS QUE HACER\n",
        "train_data, test_data = df_transformed.randomSplit([0.7, 0.3], seed=1234)\n",
        "\n",
        "#Logistic Regression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"deposit_index\")\n",
        "lr_model = lr.fit(train_data)\n",
        "lr_predictions = lr_model.transform(test_data)\n",
        "\n",
        "#Random Forest\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"deposit_index\", numTrees=100)\n",
        "rf_model = rf.fit(train_data)\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "\n",
        "#Evaluador de clasificación binaria\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"deposit_index\", rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "#Métricas para Logistic Regression\n",
        "lr_auc = evaluator.evaluate(lr_predictions)\n",
        "print(f\"Logistic Regression AUC: {lr_auc}\")\n",
        "\n",
        "#Métricas para Random Forest\n",
        "rf_auc = evaluator.evaluate(rf_predictions)\n",
        "print(f\"Random Forest AUC: {rf_auc}\")\n",
        "\n",
        "#Evaluación de Accuracy\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"deposit_index\", metricName=\"accuracy\")\n",
        "lr_accuracy = accuracy_evaluator.evaluate(lr_predictions)\n",
        "rf_accuracy = accuracy_evaluator.evaluate(rf_predictions)\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "n8psEjkNqFkq"
      },
      "outputs": [],
      "source": [
        "#8. MODELO QUE VA A PREDECIR\n",
        "# Y ACA RECIEN TENEMOS QUE HACER EL MODELO PREDICTIVO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "6pDu4tmdqFkq"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "kMl3VZoRqFkq"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain",
        "id": "wVOxB8nzqFkr"
      },
      "source": [
        "%pyspark\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain",
        "id": "7wC_0xmzqFkr"
      },
      "source": [
        "%pyspark\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain",
        "id": "UrFzGthlqFkr"
      },
      "source": [
        "%pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "vr8d2s9kqFkr"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain",
        "id": "aBGLCTAyqFkr"
      },
      "source": [
        "%pyspark\n"
      ]
    }
  ]
}