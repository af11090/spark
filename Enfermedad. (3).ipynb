{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql import SQLContext, Row\n",
        "\n",
        "# Definimos la ruta base del archivo\n",
        "ruta_base = \"file:/home/vagrant/\"\n",
        "\n",
        "# Leemos el archivo CSV como RDD, dividimos por comas, y almacenamos en caché\n",
        "data = sc.textFile(ruta_base + \"archivo.csv\").map(lambda x: x.split(\";\")).cache()\n",
        "\n",
        "# Imprimimos las primeras filas para ver los datos\n",
        "data.take(5)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Obtenemos la primera fila del RDD\n",
        "encabezado = data.first()\n",
        "\n",
        "# Imprimimos el encabezado\n",
        "print(encabezado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType, IntegerType\n",
        "\n",
        "estructura = StructType([\n",
        "    StructField('documento_anonimizado', StringType(), True),\n",
        "    StructField('periodo', IntegerType(), True),\n",
        "    StructField('mes', IntegerType(), True),\n",
        "    StructField('renaes', StringType(), True),\n",
        "    StructField('ipress', StringType(), True),\n",
        "    StructField('region', StringType(), True),\n",
        "    StructField('departamento', StringType(), True),\n",
        "    StructField('provincia', StringType(), True),\n",
        "    StructField('distrito', StringType(), True),\n",
        "    StructField('ubigeo', StringType(), True),\n",
        "    StructField('codigo_diagnostico', StringType(), True),\n",
        "    StructField('diagnosticos', StringType(), True),\n",
        "    StructField('grupo_diagnosticos', StringType(), True),\n",
        "    StructField('grupo_cobertura', StringType(), True),\n",
        "    StructField('sexo', StringType(), True),\n",
        "    StructField('edad', IntegerType(), True),\n",
        "    StructField('tipo_seguro', StringType(), True),\n",
        "    StructField('servicio', StringType(), True),\n",
        "    StructField('fecha_atencion', DateType(), True),\n",
        "    StructField('monto_bruto', FloatType(), True),\n",
        "    StructField('fecha_corte', DateType(), True)\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Para formatear las fechas\n",
        "from datetime import datetime\n",
        "def parse_date(date_string):\n",
        "    if date_string:       \n",
        "        return datetime.strptime(date_string, '%Y%m%d').date()   \n",
        "    else:      \n",
        "        return None\n",
        "        \n",
        "filas = data.filter(lambda x: x != encabezado)\n",
        "\n",
        "# Transformamos el RDD, aplicando el casteo de tipos a las columnas\n",
        "filas = filas.map(lambda x: [\n",
        "    x[0],\n",
        "    int(x[1]),\n",
        "    int(x[2]),\n",
        "    x[3],\n",
        "    x[4],\n",
        "    x[5],\n",
        "    x[6],\n",
        "    x[7],\n",
        "    x[8],\n",
        "    x[9],\n",
        "    x[10],\n",
        "    x[11],\n",
        "    x[12],\n",
        "    x[13],\n",
        "    x[14],\n",
        "    int(x[15]),\n",
        "    x[16],\n",
        "    x[17],\n",
        "    parse_date(x[18]),\n",
        "    float(x[19]),\n",
        "    parse_date(x[20])\n",
        "])\n",
        "\n",
        "filas.take(3)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df = spark.createDataFrame(filas, schema=estructura)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df.show(3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#DEPURACION\n",
        "#1. VERIFICAR EL % DE FILAS CON VALORES NULOS O ESPACIOS VACIOS\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el porcentaje de datos faltantes (nulos o vacíos) para cada columna\n",
        "porcentaje_faltantes = [(col, (df.filter((F.col(col).isNull()) | (F.col(col) == \"\") | (F.trim(F.col(col)) == \"\")).count() / df.count()) * 100) for col in df.columns]\n",
        "\n",
        "# Convertir el resultado a un DataFrame sin redondeo\n",
        "df_porcentaje = spark.createDataFrame(porcentaje_faltantes, [\"Columna\", \"Porcentaje_Faltantes\"])\n",
        "\n",
        "# Mostrar el DataFrame con todos los decimales\n",
        "df_porcentaje.show(21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(f\"Número de filas original: {df.count()}\")\n",
        "\n",
        "# Crear una condición combinada para identificar filas con valores nulos o vacíos\n",
        "condicion = None\n",
        "for col in df.columns:\n",
        "    nueva_condicion = (F.col(col).isNull()) | (F.trim(F.col(col)) == \"\")  # Verificar nulos o espacios vacíos\n",
        "    if condicion is None:\n",
        "        condicion = nueva_condicion\n",
        "    else:\n",
        "        condicion = condicion | nueva_condicion\n",
        "\n",
        "# Filtrar las filas que NO cumplen con la condición (es decir, eliminar las filas con nulos o vacíos)\n",
        "df = df.filter(~condicion)\n",
        "\n",
        "# Mostrar el número de filas después de eliminar las filas con nulos o vacíos\n",
        "print(f\"Número de filas después de la limpieza: {df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(f\"Número de filas original sin vacios: {df.count()}\")\n",
        "df = df[df['GRUPO_DIAGNOSTICOS'] == 'ENFERMEDAD RARA O HUERFANA']\n",
        "print(f\"Número de filas original con la ENFERMEDAD RARA: {df.count()}\")\n",
        "#print(df['GRUPO_DIAGNOSTICOS'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#1. DEPURACIÓN DE FORMATO Y CONSISTENCIA DE LAS FECHAS\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "# Depuración de formato y consistencia de las fechas\n",
        "df = df.withColumn(\"FECHA_ATENCION_VALID\", to_date(col(\"FECHA_ATENCION\"), \"yyyyMMdd\").isNotNull())\n",
        "df = df.withColumn(\"FECHA_CORTE_VALID\", to_date(col(\"FECHA_CORTE\"), \"yyyyMMdd\").isNotNull())\n",
        "\n",
        "# Creación de una tabla resumen con la cantidad de fechas mal formateadas\n",
        "df_summary = df.agg(\n",
        "    F.sum(F.when(~col(\"FECHA_ATENCION_VALID\"), 1).otherwise(0)).alias(\"Fechas_ATENCION_mal_formateadas\"),\n",
        "    F.sum(F.when(~col(\"FECHA_CORTE_VALID\"), 1).otherwise(0)).alias(\"Fechas_CORTE_mal_formateadas\"),\n",
        "    F.count(\"*\").alias(\"Total_filas\")\n",
        ")\n",
        "\n",
        "df_summary.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(f\"Número de filas original sin vacios y solo con ENFERMEDAD: {df.count()}\")\n",
        "\n",
        "#2. DEPURACION Verificar valores inconsistentes de las edades\n",
        "df = df.filter((col(\"EDAD\") >= 0) & (col(\"EDAD\") <= 110))\n",
        "print(f\"Número de filas después de la limpieza de edad: {df.count()}\")\n",
        "\n",
        "#3. Verificar que MONTO_BRUTO sea un valor positivo\n",
        "df=df.filter(col(\"MONTO_BRUTO\") >= 0)\n",
        "print(f\"Número de filas después de la limpieza del monto: {df.count()}\")\n",
        "\n",
        "#4. DEPURACION Verificar valores inconsistentes de los meses\n",
        "df=df.filter((col(\"MES\") > 0) | (col(\"MES\") <= 12))\n",
        "print(f\"Número de filas después de la limpieza del mes: {df.count()}\")\n",
        "\n",
        "#4. Verificar valores inconsistentes en la columna SEXO\n",
        "df.select(\"SEXO\").distinct().show()\n",
        "\n",
        "#5. Verificar valores inconsistentes en la columna TIPO_SEGURO\n",
        "df.select(\"TIPO_SEGURO\").distinct().show()\n",
        "\n",
        "#5. Verificar valores inconsistentes en la columna TIPO_SEGURO\n",
        "df.select(\"REGION\").distinct().show(24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#6. Eliminar filas duplicadas, si es que lo hay\n",
        "filas_antes = df.count()\n",
        "df = df.dropDuplicates()\n",
        "filas_despues = df.count()\n",
        "\n",
        "print(f\"Filas antes de eliminar duplicados: {filas_antes}\")\n",
        "print(f\"Filas después de eliminar duplicados: {filas_despues}\")\n",
        "print(f\"Cantidad de duplicados eliminados: {filas_antes - filas_despues}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Código de verificación\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el porcentaje de datos faltantes (nulos o vacíos) para cada columna\n",
        "porcentaje_faltantes = [(col, (df.filter((F.col(col).isNull()) | (F.col(col) == \"\") | (F.trim(F.col(col)) == \"\")).count() / df.count()) * 100) for col in df.columns]\n",
        "\n",
        "# Convertir el resultado a un DataFrame sin redondeo\n",
        "df_porcentaje = spark.createDataFrame(porcentaje_faltantes, [\"Columna\", \"Porcentaje_Faltantes\"])\n",
        "\n",
        "# Mostrar el DataFrame con todos los decimales\n",
        "df_porcentaje.show(21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from unidecode import unidecode\n",
        "from pyspark.sql.functions import udf,col\n",
        "# Función UDF para limpiar caracteres especiales\n",
        "def clean_string(s):\n",
        "    return unidecode(s) if s else s\n",
        "\n",
        "clean_string_udf = udf(clean_string, StringType())\n",
        "\n",
        "# Aplicar la función UDF a las columnas que necesitan limpieza\n",
        "columnas_a_limpiar = ['ipress', 'region', 'departamento', 'provincia', 'diagnosticos']\n",
        "\n",
        "for columna in columnas_a_limpiar:\n",
        "    df = df.withColumn(columna, clean_string_udf(col(columna)))\n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#CREACION DE TABLAS ADICIONALES\n",
        "from pyspark.sql import SQLContext, Row\n",
        "\n",
        "# Definimos la ruta base del archivo\n",
        "ruta_base2 = \"file:/home/vagrant/\"\n",
        "\n",
        "# Leemos el archivo CSV como RDD, dividimos por comas, y almacenamos en caché\n",
        "data2 = sc.textFile(ruta_base + \"tabla1.csv\").map(lambda x: x.split(\";\")).cache()\n",
        "\n",
        "# Imprimimos las primeras filas para ver los datos\n",
        "data2.take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Obtenemos la primera fila del RDD\n",
        "encabezado2 = data2.first()\n",
        "\n",
        "# Imprimimos el encabezado\n",
        "print(encabezado2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType, IntegerType\n",
        "\n",
        "estructura2 = StructType([\n",
        "    StructField('FECHA_CORTE', DateType(), True),\n",
        "    StructField('DEPARTAMENTO', StringType(), True),\n",
        "    StructField('PROVINCIA', StringType(), True),\n",
        "    StructField('DISTRITO', StringType(), True),\n",
        "    StructField('UBIGEO', StringType(), True),\n",
        "    StructField('RED', StringType(), True),\n",
        "    StructField('IPRESS', StringType(), True),\n",
        "    StructField('ID_PACIENTE', StringType(), True),\n",
        "    StructField('EDAD_PACIENTE', IntegerType(), True),\n",
        "    StructField('SEXO_PACIENTE', StringType(), True),\n",
        "    StructField('EDAD_MEDICO', IntegerType(), True),\n",
        "    StructField('ID_MEDICO', StringType(), True),\n",
        "    StructField('COD_DIAG', StringType(), True),\n",
        "    StructField('DIAGNOSTICO', StringType(), True),\n",
        "    StructField('AREA_HOSPITALARIA', StringType(), True),\n",
        "    StructField('SERVICIO_HOSPITALARIO', StringType(), True),\n",
        "    StructField('ACTIVIDAD_HOSPITALARIA', StringType(), True),\n",
        "    StructField('FECHA_MUESTRA', DateType(), True),\n",
        "    StructField('FEC_RESULTADO_1', DateType(), True),\n",
        "    StructField('DIFERIMIENTO_1', IntegerType(), True),\n",
        "    StructField('PROCEDIMIENTO_1', StringType(), True),\n",
        "    StructField('RESULTADO_1', FloatType(), True),\n",
        "    StructField('UNIDADES_1', StringType(), True),\n",
        "    StructField('FEC_RESULTADO_2', DateType(), True),\n",
        "    StructField('PROCEDIMIENTO_2', StringType(), True),\n",
        "    StructField('RESULTADO_2', FloatType(), True),\n",
        "    StructField('UNIDADES_2', StringType(), True),\n",
        "    StructField('DIFERIMIENTO_2', IntegerType(), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from datetime import datetime\n",
        "\n",
        "def parse_date2(date_string):\n",
        "    if date_string:\n",
        "        return datetime.strptime(date_string, '%Y%m%d').date()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "filas2 = data2.filter(lambda x: x != encabezado2)\n",
        "\n",
        "# Transformamos el RDD, aplicando el casteo de tipos a las columnas\n",
        "filas2 = filas2.map(lambda x: [\n",
        "    parse_date2(x[0]),  # FECHA_CORTE\n",
        "    x[1],  # DEPARTAMENTO\n",
        "    x[2],  # PROVINCIA\n",
        "    x[3],  # DISTRITO\n",
        "    x[4],  # UBIGEO\n",
        "    x[5],  # RED\n",
        "    x[6],  # IPRESS\n",
        "    x[7],  # ID_PACIENTE\n",
        "    int(x[8]),  # EDAD_PACIENTE\n",
        "    x[9],  # SEXO_PACIENTE\n",
        "    int(x[10]),  # EDAD_MEDICO\n",
        "    x[11],  # ID_MEDICO\n",
        "    x[12],  # COD_DIAG\n",
        "    x[13],  # DIAGNOSTICO\n",
        "    x[14],  # AREA_HOSPITALARIA\n",
        "    x[15],  # SERVICIO_HOSPITALARIO\n",
        "    x[16],  # ACTIVIDAD_HOSPITALARIA\n",
        "    parse_date2(x[17]),  # FECHA_MUESTRA\n",
        "    parse_date2(x[18]),  # FEC_RESULTADO_1\n",
        "    int(x[19]),  # DIFERIMIENTO_1\n",
        "    x[20],  # PROCEDIMIENTO_1\n",
        "    float(x[21]),  # RESULTADO_1\n",
        "    x[22],  # UNIDADES_1\n",
        "    parse_date2(x[23]),  # FEC_RESULTADO_2\n",
        "    x[24],  # PROCEDIMIENTO_2\n",
        "    float(x[25]),  # RESULTADO_2\n",
        "    x[26],  # UNIDADES_2\n",
        "    int(x[27])  # DIFERIMIENTO_2\n",
        "])\n",
        "\n",
        "filas2.take(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df2 = spark.createDataFrame(filas2, schema=estructura2)\n",
        "df2.show(5)\n",
        "\n",
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Seleccionamos las columnas Diagnóstico, procedimiento_1 y resultado_1 desde el RDD\n",
        "data_rdd2 = data2.map(lambda row: (row[13], row[20], row[21]))\n",
        "\n",
        "# Mostramos las primeras filas de los resultados\n",
        "data_rdd2.take(5)  # Aquí puedes ajustar el número de filas que deseas visualizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Obtenemos el encabezado (primera fila) del RDD\n",
        "encabezado2 = data2.first()\n",
        "\n",
        "\n",
        "# Eliminamos la primera fila (encabezado) y seleccionamos solo las columnas necesarias\n",
        "data_rdd2 = data2.filter(lambda row: row != encabezado2).map(lambda row: (row[8], row[13], row[20], row[21]))  # Usamos los índices de las columnas necesarias\n",
        "\n",
        "# Convertimos el RDD a una lista de tuplas\n",
        "data_list = data_rdd2.collect()\n",
        "\n",
        "# Definir las columnas utilizando solo los índices necesarios para 'Diagnóstico', 'Procedimiento_1' y 'Resultado_1'\n",
        "columns = ['EDAD','DIAGNOSTICO', 'PROCEDIMIENTO_1', 'RESULTADO_1']\n",
        "\n",
        "# Creamos el DataFrame usando los nombres de las columnas correctos\n",
        "df_tabla1 = spark.createDataFrame(data_list, columns)\n",
        "\n",
        "# Mostramos el DataFrame\n",
        "df_tabla1.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Obtenemos el encabezado (primera fila) del RDD\n",
        "encabezado2 = data2.first()\n",
        "\n",
        "\n",
        "# Eliminamos la primera fila (encabezado) y seleccionamos solo las columnas necesarias\n",
        "data_rdd2 = data2.filter(lambda row: row != encabezado2).map(lambda row: (row[1], row[14], row[15], row [16]))  # Usamos los índices de las columnas necesarias\n",
        "\n",
        "# Convertimos el RDD a una lista de tuplas\n",
        "data_list = data_rdd2.collect()\n",
        "\n",
        "columns = ['DEPARTAMENTO', 'AREA_HOSPITALARIA', 'SERVICIO_HOSPITALARIO', 'ACTIVIDAD_HOSPITALARIA']\n",
        "\n",
        "# Creamos el DataFrame usando los nombres de las columnas correctos\n",
        "df_tabla2 = spark.createDataFrame(data_list, columns)\n",
        "\n",
        "# Mostramos el DataFrame\n",
        "df_tabla2.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Creacion de vistas temporales\n",
        "\n",
        "df1 = df.createTempView('Vista01')\n",
        "df2 = df.createTempView('Vista02')\n",
        "df3 = df.createTempView('Vista03')\n",
        "df4 = df.createTempView('Vista04')\n",
        "df5 = df.createTempView('Vista05')\n",
        "df6 = df.createTempView('Vista06')\n",
        "df7 = df.createTempView('Vista07')\n",
        "df8 = df.createTempView('Vista08')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#CONSULTAS\n",
        "#Indicador 1: Costo Promedio de Atención de Diagnóstico por sexo\n",
        "#Costo Promedio de Atención de Diagnóstico por Sexo y Edad (segmentado por ERH)\n",
        "rpta1 = spark.sql(\"\"\"\n",
        "SELECT \n",
        "    SEXO,\n",
        "    EDAD,\n",
        "    AVG(MONTO_BRUTO) AS Costo_Promedio\n",
        "FROM \n",
        "    Vista01 \n",
        "WHERE \n",
        "    grupo_cobertura = 'ERH'\n",
        "GROUP BY \n",
        "    SEXO, \n",
        "    EDAD\n",
        "ORDER BY \n",
        "    SEXO, \n",
        "    EDAD\n",
        "\"\"\")\n",
        "\n",
        "#Mostrar resultado\n",
        "rpta1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Indicador 1: Costo Promedio de Atención de Diagnóstico por sexo\n",
        "#Distribución de Edades por Sexo\n",
        "\n",
        "rpta2 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    sexo,\n",
        "    edad,\n",
        "    COUNT(*) AS numero_pacientes\n",
        "FROM\n",
        "    Vista02\n",
        "GROUP BY\n",
        "    sexo,\n",
        "    edad\n",
        "ORDER BY\n",
        "    sexo,\n",
        "    edad\n",
        "\"\"\")\n",
        "#Mostrar resultado\n",
        "rpta2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Indicador 2: Porcentaje de Diagnósticos por Sexo\n",
        "#Distribución de Edades por Sexo #DUDA\n",
        "rpta3 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    grupo_cobertura,\n",
        "    (COUNT(*) / (SELECT COUNT(*) FROM Vista08)) * 100 AS porcentaje_atenciones\n",
        "FROM\n",
        "    Vista08\n",
        "WHERE\n",
        "    grupo_diagnosticos = 'ENFERMEDAD RARA O HUERFANA'  -- Asegurarse de filtrar solo las ERH\n",
        "GROUP BY\n",
        "    grupo_cobertura\n",
        "ORDER BY\n",
        "    porcentaje_atenciones DESC  -- Ordenar para ver los grupos de cobertura más frecuentes\n",
        "\"\"\")\n",
        "\n",
        "# Mostrar resultado\n",
        "rpta3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Indicador 2: Porcentaje de Diagnósticos por Sexo\n",
        "# Top 10 Diagnósticos Más Comunes\n",
        "\n",
        "rpta4 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    diagnosticos,\n",
        "    COUNT(*) AS numero_diagnosticos\n",
        "FROM\n",
        "    Vista08\n",
        "GROUP BY\n",
        "    diagnosticos\n",
        "ORDER BY\n",
        "    numero_diagnosticos DESC\n",
        "LIMIT 10\n",
        "\"\"\")\n",
        "\n",
        "#Mostrar resultado\n",
        "rpta4.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Indicador 3: Costo promedio de diagnóstico por tipo de seguro \n",
        "# Consulta: Evolución del Costo Promedio Mensual por Tipo de Seguro\n",
        "\n",
        "rpta5 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    periodo,\n",
        "    mes,\n",
        "    tipo_seguro,\n",
        "    AVG(monto_bruto) AS costo_promedio\n",
        "FROM\n",
        "    Vista08\n",
        "GROUP BY\n",
        "    periodo,\n",
        "    mes,\n",
        "    tipo_seguro\n",
        "ORDER BY\n",
        "    periodo,\n",
        "    mes,\n",
        "    tipo_seguro\n",
        "\"\"\")\n",
        "\n",
        "#Mostrar resultado\n",
        "rpta5.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Indicador 3: Costo promedio de diagnóstico por tipo de seguro \n",
        "#9 Monto Bruto promedio por region y sexo\n",
        "\n",
        "rpta6 = spark.sql(\"\"\"\n",
        "SELECT region, sexo, AVG(monto_bruto) AS promedio_monto_bruto FROM Vista08 GROUP BY region, sexo\n",
        "\"\"\")\n",
        "\n",
        "#Mostrar resultado\n",
        "rpta6.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Indicador 4: Porcentaje de Atención por Región \n",
        "# Consulta: Porcentaje de Atenciones de ERH por Grupo de Cobertura #DUDAR\n",
        "\n",
        "rpta7 = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    grupo_cobertura,\n",
        "    (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM Vista08)) AS porcentaje_atenciones\n",
        "FROM\n",
        "    Vista08\n",
        "WHERE\n",
        "    grupo_diagnosticos = 'ENFERMEDAD RARA O HUERFANA'  -- Asegurarse de filtrar solo las ERH\n",
        "GROUP BY\n",
        "    grupo_cobertura\n",
        "ORDER BY\n",
        "    porcentaje_atenciones DESC  -- Ordenar para ver los grupos de cobertura más frecuentes\n",
        "\"\"\")\n",
        "# Mostrar resultado\n",
        "rpta7.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#Indicador 4: Porcentaje de Atención por Región \n",
        "# Consulta: Cantidad de atenciones por mes\n",
        "rpta8 = spark.sql(\"\"\"\n",
        "SELECT mes, COUNT(*) AS total_atenciones FROM Vista08 GROUP BY mes ORDER BY mes\n",
        "\"\"\")\n",
        "\n",
        "rpta8 = spark.sql(\"\"\"\n",
        "SELECT mes, COUNT(*) AS total_atenciones FROM Vista08 GROUP BY mes ORDER BY mes\n",
        "\"\"\")\n",
        "\n",
        "#Mostrar resultado\n",
        "rpta8.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# INDICADOR 1\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import sum as _sum, count, round, col\n",
        "\n",
        "# INDICADOR 1: Agrupación por GRUPO_COBERTURA, SEXO y DIAGNOSTICOS con orden alfabético\n",
        "resultado1 = df.groupBy(\"SEXO\", \"DIAGNOSTICOS\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"TOTAL_ATENCIONES\"),\n",
        "        round((_sum(\"MONTO_BRUTO\") / count(\"*\")), 2).alias(\"COSTO_PROMEDIO_ATENCION\")\n",
        "    ) \\\n",
        "    .orderBy(\"DIAGNOSTICOS\")\n",
        "\n",
        "print(\"COSTO PROMEDIO Y TOTAL DE ATENCIONES POR SEXO:\")\n",
        "resultado1.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#INDICADOR 2\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el número de diagnósticos por sexo\n",
        "diagnosticos_por_sexo = df.groupBy(\"SEXO\").agg(\n",
        "    F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_DIAGNOSTICOS\")\n",
        ")\n",
        "\n",
        "# Calcular el total general de diagnósticos\n",
        "total_diagnosticos = df.select(F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_GENERAL\")).collect()[0][\"TOTAL_GENERAL\"]\n",
        "\n",
        "# Calcular el porcentaje de diagnósticos por sexo\n",
        "porcentaje_diagnosticos = diagnosticos_por_sexo.withColumn(\n",
        "    \"PORCENTAJE_DIAGNOSTICOS(%)\",\n",
        "    F.round((F.col(\"TOTAL_DIAGNOSTICOS\") / total_diagnosticos) * 100, 2)\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por SEXO\n",
        "porcentaje_diagnosticos.orderBy(\"SEXO\").show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#INDICADOR 3\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, sum as _sum, count, round\n",
        "\n",
        "# Calcular el costo promedio y el total de atenciones agrupando por tipo de seguro y diagnóstico\n",
        "resultado3 = df.groupBy(\"TIPO_SEGURO\", \"DIAGNOSTICOS\").agg(\n",
        "    count(\"*\").alias(\"TOTAL_ATENCIONES\"),  # Total de atenciones\n",
        "    round(_sum(\"MONTO_BRUTO\") / count(\"*\"), 2).alias(\"COSTO_PROMEDIO\")  # Costo promedio usando _sum\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por tipo de seguro\n",
        "resultado3.orderBy(\"TIPO_SEGURO\").show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#INDICADOR 4\n",
        "# Contamos el número total de atenciones en todas las regiones\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "total_atenciones = df.count()\n",
        "\n",
        "# Contamos el número de atenciones por cada región\n",
        "atenciones_por_region = df.groupBy(\"REGION\").count().withColumnRenamed(\"count\", \"TOTAL_ATENCIONES\")\n",
        "\n",
        "# Calculamos la tasa de atención por región y redondeamos en una sola operación\n",
        "tasa_atencion_region = atenciones_por_region.withColumn(\n",
        "    \"Tasa de Atención (%)\", \n",
        "    F.round((col(\"TOTAL_ATENCIONES\") / total_atenciones) * 100, 2)\n",
        ")\n",
        "\n",
        "# Mostramos el resultado\n",
        "tasa_atencion_region.show(24)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#ESTADISTICA DESCRIPTIVA\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "# Calcular las estadísticas numéricas\n",
        "numerical_stats = df.select(\"EDAD\", \"MONTO_BRUTO\") \\\n",
        "    .summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\")\n",
        "\n",
        "# Redondear a 2 decimales\n",
        "numerical_stats_rounded = numerical_stats.select(\n",
        "    \"summary\",\n",
        "    round(\"EDAD\", 2).alias(\"EDAD\"),\n",
        "    round(\"MONTO_BRUTO\", 2).alias(\"MONTO_BRUTO\")\n",
        ")\n",
        "\n",
        "# Mostrar el resultado\n",
        "numerical_stats_rounded.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#CORRELACION ENTRE DATOS\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "# Preparar datos para correlación\n",
        "df = df.withColumn(\"EDAD\", col(\"EDAD\").cast(\"integer\"))\n",
        "df = df.withColumn(\"MONTO_BRUTO\", col(\"MONTO_BRUTO\").cast(\"float\"))\n",
        "\n",
        "vector_cols = [\"EDAD\", \"MONTO_BRUTO\"]\n",
        "assembler = VectorAssembler(inputCols=vector_cols, outputCol=\"features\")\n",
        "df_vector = assembler.transform(df)\n",
        "\n",
        "\n",
        "# Calcular matriz de correlación\n",
        "matrix = Correlation.corr(df_vector, \"features\").head()[0]\n",
        "correlation_matrix = matrix.toArray().tolist()\n",
        "\n",
        "for row in correlation_matrix:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#ANALISIS ESTADISTICO\n",
        "# KPI 1: Análisis detallado \n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, count, round, avg, stddev, sum as _sum, when\n",
        "\n",
        "# Calcular métricas detalladas\n",
        "resultado1_detallado = df.groupBy(\"SEXO\", \"DIAGNOSTICOS\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"TOTAL_ATENCIONES\"),  # Total de atenciones\n",
        "        round(avg(\"MONTO_BRUTO\"), 2).alias(\"COSTO_PROMEDIO_ATENCION\"),  # Promedio de MONTO_BRUTO\n",
        "        # Si el total de atenciones es 1, la desviación estándar será 0\n",
        "        round(when(count(\"*\") == 1, 0).otherwise(stddev(\"MONTO_BRUTO\")), 2).alias(\"DESVIACION_ESTANDAR_MONTO\"),  \n",
        "        round(avg(\"EDAD\"), 2).alias(\"PROMEDIO_EDAD\")  # Promedio de la edad\n",
        "    ) \\\n",
        "    .orderBy(\"DIAGNOSTICOS\")\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"ANÁLISIS DETALLADO: COSTO PROMEDIO, DESVIACIÓN ESTÁNDAR Y PROMEDIO DE EDAD POR SEXO Y DIAGNÓSTICO:\")\n",
        "resultado1_detallado.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#INDICADOR 2: ANALISIS DETALLADO\n",
        "#INDICADOR 2: SEXO, TOTAL DE DIAGNOSTICOS, PORCENTAJE, EDAD(PROMEDIO), MONTO_BRUTO(PROMEDIO Y DES.EST)\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calcular el número de diagnósticos, edad promedio, monto promedio y desviación estándar por sexo\n",
        "diagnosticos_detallado = df.groupBy(\"SEXO\").agg(\n",
        "    F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_DIAGNOSTICOS\"),               # Total de diagnósticos\n",
        "    F.round(F.avg(\"EDAD\"), 2).alias(\"EDAD_PROMEDIO\"),                  # Promedio de edad\n",
        "    F.round(F.avg(\"MONTO_BRUTO\"), 2).alias(\"PROMEDIO_MONTO_BRUTO\"),    # Promedio del monto bruto\n",
        "    F.round(F.stddev(\"MONTO_BRUTO\"), 2).alias(\"DESVIACION_MONTO_BRUTO\")# Desviación estándar del monto bruto\n",
        ")\n",
        "\n",
        "# Calcular el total general de diagnósticos\n",
        "total_diagnosticos = df.select(F.count(\"DIAGNOSTICOS\").alias(\"TOTAL_GENERAL\")).collect()[0][\"TOTAL_GENERAL\"]\n",
        "\n",
        "# Calcular el porcentaje de diagnósticos por sexo\n",
        "diagnosticos_con_porcentaje = diagnosticos_detallado.withColumn(\n",
        "    \"PORCENTAJE_DIAGNOSTICOS(%)\",\n",
        "    F.round((F.col(\"TOTAL_DIAGNOSTICOS\") / total_diagnosticos) * 100, 2)\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por SEXO\n",
        "diagnosticos_con_porcentaje.orderBy(\"SEXO\").show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#INDICADOR 3: ANALISIS DETALLADO\n",
        "#INDICADOR 3: TIPO DE SEGURO, DIAGNOSTICO, MONTO(PROMEDIO Y DESV.), TOTAL DE ATENCIONES, EDAD(PROMEDIO)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, sum as _sum, count, round, avg, stddev, when\n",
        "\n",
        "# Calcular el costo promedio, la desviación estándar y el promedio de edad agrupando por tipo de seguro y diagnóstico\n",
        "resultado_detallado = df.groupBy(\"TIPO_SEGURO\", \"DIAGNOSTICOS\").agg(\n",
        "    count(\"*\").alias(\"TOTAL_ATENCIONES\"),  # Total de atenciones\n",
        "    round(avg(\"MONTO_BRUTO\"), 2).alias(\"PROMEDIO_MONTO_BRUTO\"),  # Promedio del MONTO_BRUTO\n",
        "    # Si el total de atenciones es 1, la desviación estándar será 0\n",
        "    round(when(count(\"*\") == 1, 0).otherwise(stddev(\"MONTO_BRUTO\")), 2).alias(\"DESVIACION_MONTO_BRUTO\"),  \n",
        "    round(avg(\"EDAD\"), 2).alias(\"PROMEDIO_EDAD\")  # Promedio de la edad\n",
        ")\n",
        "\n",
        "# Mostrar el resultado ordenado por tipo de seguro y diagnóstico\n",
        "resultado_detallado.orderBy(\"TIPO_SEGURO\", \"DIAGNOSTICOS\").show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#INDICADOR 4: ANALISIS DETALLADO\n",
        "#INDICADOR 4: REGION, TOTAL DE ATENCIONES, PORCENTAJE, EDAD(PROMEDIO), MONTO_BRUTO(PROMEDIO Y DES. ESTA)\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, avg, stddev, round, count, when\n",
        "\n",
        "# Contar el número total de atenciones en todas las regiones\n",
        "total_atenciones = df.count()\n",
        "\n",
        "# Agrupamos por región, calculamos las métricas y ajustamos la desviación estándar si hay solo una atención\n",
        "analisis_region = df.groupBy(\"REGION\").agg(\n",
        "    count(\"*\").alias(\"TOTAL_ATENCIONES\"),\n",
        "    round(avg(\"EDAD\"), 2).alias(\"PROMEDIO_EDAD\"),\n",
        "    round(avg(\"MONTO_BRUTO\"), 2).alias(\"PROMEDIO_MONTO_BRUTO\"),\n",
        "    round(when(count(\"*\") == 1, 0).otherwise(stddev(\"MONTO_BRUTO\")), 2).alias(\"DESVIACION_MONTO_BRUTO\")\n",
        ").withColumn(\n",
        "    \"PORCENTAJE_ATENCIONES(%)\", \n",
        "    round((col(\"TOTAL_ATENCIONES\") / total_atenciones) * 100, 2)\n",
        ").orderBy(col(\"PORCENTAJE_ATENCIONES(%)\").desc())\n",
        "\n",
        "# Mostrar el resultado\n",
        "analisis_region.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#GRAFICOS\n",
        "#INDICADOR 1\n",
        "import matplotlib.pyplot as plt\n",
        "# Convertir el DataFrame de Spark en un DataFrame de pandas\n",
        "data_kpi = resultado1.orderBy(\"COSTO_PROMEDIO_ATENCION\", ascending=False).toPandas()\n",
        "\n",
        "# Seleccionar los 20 diagnósticos con mayor costo promedio\n",
        "top_data = data_kpi.head(20)\n",
        "\n",
        "# Crear el gráfico de barras horizontales, diferenciando por SEXO\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Colorear las barras según el SEXO (azul para masculino, rosa para femenino)\n",
        "for sexo in top_data['SEXO'].unique():\n",
        "    subset = top_data[top_data['SEXO'] == sexo]\n",
        "    plt.barh(subset[\"DIAGNOSTICOS\"], subset[\"COSTO_PROMEDIO_ATENCION\"], label=sexo, alpha=0.7)\n",
        "\n",
        "# Añadir etiquetas y título\n",
        "plt.xlabel(\"Costo Promedio de Atención\")\n",
        "plt.ylabel(\"Diagnósticos\")\n",
        "plt.title(\"Top 20 Diagnósticos con Mayor Costo Promedio de Atención por Sexo\")\n",
        "\n",
        "# Invertir el eje Y para que el diagnóstico con mayor costo esté arriba\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Añadir una leyenda para SEXO\n",
        "plt.legend(title=\"Sexo\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# INDICADOR 2:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convertir el DataFrame de PySpark a Pandas\n",
        "data_sexo = porcentaje_diagnosticos.orderBy(\"SEXO\").toPandas()\n",
        "\n",
        "# Crear el gráfico de barras\n",
        "plt.figure(figsize=(10, 6))  # Ajusté el tamaño para mejor visualización\n",
        "plt.bar(data_sexo[\"SEXO\"], data_sexo[\"PORCENTAJE_DIAGNOSTICOS(%)\"], color=\"salmon\")\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.xlabel(\"Sexo\")\n",
        "plt.ylabel(\"Porcentaje de Diagnósticos (%)\")\n",
        "plt.title(\"Porcentaje de Diagnósticos por Sexo\")\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.xticks(rotation=0)  # No es necesario rotar ya que solo son dos categorías\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Convertir el DataFrame de Spark en un DataFrame de pandas\n",
        "data_kpi3_1 = resultado3.orderBy(\"COSTO_PROMEDIO\", ascending=False).toPandas()\n",
        "\n",
        "# Seleccionar los 20 diagnósticos con mayor costo promedio\n",
        "top_data3_1 = data_kpi3_1.head(20)\n",
        "\n",
        "# Crear el gráfico de barras agrupadas por TIPO_SEGURO para cada DIAGNOSTICO\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Crear barras agrupadas por TIPO_SEGURO para cada DIAGNOSTICO\n",
        "for tipo_seguro in top_data3_1[\"TIPO_SEGURO\"].unique():\n",
        "    subset = top_data3_1[top_data3_1[\"TIPO_SEGURO\"] == tipo_seguro]\n",
        "    plt.barh(subset[\"DIAGNOSTICOS\"], subset[\"COSTO_PROMEDIO\"], label=tipo_seguro, alpha=0.7)\n",
        "\n",
        "# Añadir etiquetas y título\n",
        "plt.xlabel(\"Costo Promedio\")\n",
        "plt.ylabel(\"Diagnósticos\")\n",
        "plt.title(\"Top 20 Diagnósticos con Mayor Costo Promedio por Tipo de Seguro\")\n",
        "\n",
        "# Invertir el eje Y para que el diagnóstico con mayor costo esté arriba\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Añadir una leyenda para los tipos de seguro\n",
        "plt.legend(title=\"Tipo de Seguro\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# INDICADOR 4:\n",
        "# Mostrar tasa de atención por \"REGION\"\n",
        "data_region = tasa_atencion_region.orderBy(\"Tasa de Atención (%)\").toPandas()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(data_region[\"REGION\"], data_region[\"Tasa de Atención (%)\"], color=\"violet\")\n",
        "plt.xlabel(\"Región\")\n",
        "plt.ylabel(\"Tasa de Atención (%)\")\n",
        "plt.title(\"Tasa de Atención por Región\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#ANALISIS ESTADISTICO INTEGRADO\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, count, round, avg, stddev, sum as _sum, when, countDistinct\n",
        "\n",
        "analisis_integrado = df.groupBy(\"TIPO_SEGURO\").agg(\n",
        "    count(\"*\").alias(\"TOTAL_ATENCIONES\"),\n",
        "    countDistinct(\"DIAGNOSTICOS\").alias(\"TOTAL_DIAGNOSTICOS\"),\n",
        "    count(when(col(\"SEXO\") == \"FEMENINO\", True)).alias(\"TOTAL_ATENCIONES_FEMENINO\"),\n",
        "    count(when(col(\"SEXO\") == \"MASCULINO\", True)).alias(\"TOTAL_ATENCIONES_MASCULINO\"),\n",
        "    round(avg(\"MONTO_BRUTO\"), 2).alias(\"PROMEDIO_MONTO_BRUTO\"),\n",
        "    round(when(count(\"*\") == 1, 0).otherwise(stddev(\"MONTO_BRUTO\")), 2).alias(\"DESVIACION_ESTANDAR_MONTO\"),\n",
        "    round(avg(\"EDAD\"), 2).alias(\"PROMEDIO_EDAD\")\n",
        ").orderBy(\"TIPO_SEGURO\")\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"ANÁLISIS INTEGRADO:\")\n",
        "analisis_integrado.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#METRICAS DE RENDIMIENTO\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, count, round, avg, stddev\n",
        " \n",
        "# Calcula las métricas de rendimiento global\n",
        "performance_metrics = df.agg(\n",
        "    countDistinct(\"DIAGNOSTICOS\").alias(\"Total Diagnósticos\"),  # Total de diagnósticos únicos\n",
        "    round(avg(\"MONTO_BRUTO\"), 2).alias(\"Promedio Monto Bruto\"),  # Promedio del monto bruto\n",
        "    round(stddev(\"MONTO_BRUTO\"), 2).alias(\"Desviación Estándar Monto\"),  # Desviación estándar del monto bruto\n",
        "    round(avg(\"EDAD\"), 2).alias(\"Promedio Edad\"),  # Promedio de la edad\n",
        "    count(\"*\").alias(\"Total Atenciones\") #Total de atenciones\n",
        ").collect()[0].asDict()\n",
        " \n",
        "print(\"\\n=== Métricas de Rendimiento Global ===\")\n",
        "for field, value in performance_metrics.items(): #Iterar sobre el diccionario\n",
        "    print(f\"{field}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#MODELO PREDICTIVO\n",
        "# 1. Distribución de la variable objetivo\n",
        "if 'MONTO_BRUTO' in df.columns:\n",
        "    df.groupBy('MONTO_BRUTO').count().show()\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        " \n",
        "# Lista de columnas a eliminar\n",
        "columns_to_drop = ['PERIODO','DOCUMENTO_ANONIMIZADO', 'RENAES', 'DISTRITO', 'UBIGEO', 'SERVICIO', 'CODIGO_DIAGNOSTICO']\n",
        " \n",
        "# Eliminar las columnas del DataFrame\n",
        "df = df.drop(*columns_to_drop)\n",
        "print(df.columns)\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#2. Análisis estadístico de columnas numéricas\n",
        "df.describe().show()\n",
        "\n",
        "# 4. Verificar correlaciones entre columnas numéricas\n",
        "numeric_columns= [col_name for col_name, dtype in df.dtypes if dtype in ['int', 'double', 'float']]\n",
        "if numeric_columns:\n",
        "    assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"features_numeric\")\n",
        "    df_assembled = assembler.transform(df)\n",
        "    correlation_matrix = Correlation.corr(df_assembled, \"features_numeric\").head()[0]\n",
        "    print(f\"Matriz de correlaciones (ordenada por columnas numércias):\")\n",
        "    print (correlation_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(df.columns)\n",
        "\n",
        " #4. Evaluación de categorías en columnas categóricas\n",
        "categorical_columns = [col_name for col_name, dtype in df.dtypes if dtype == 'string']\n",
        "for col_name in categorical_columns:\n",
        "    unique_count = df.select(col_name).distinct().count()\n",
        "    print(f\"Columna categórica {col_name}' tiene {unique_count} valores únicos.\")\n",
        "\n",
        " \n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#5. Recomendación preliminar del modelo basado en análisis de datos\n",
        "if len(numeric_columns) > 3 and df.select('MONTO_BRUTO').distinct().count() == 2:\n",
        "    print(\"Sugerencia: Modelos de clasificación binaria como Logistic Regression o Random Forest son adecuados.\")\n",
        "elif len(numeric_columns) > 3:\n",
        "  print(\"Sugerencia: Modelos de regresión como Linear Regression podrían ser útiles.\")\n",
        "else:\n",
        "  print(\"Sugerencia: Realiza más análisis para identificar relaciones complejas y considerar modelos no lineales.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Asumiendo que 'df' ya está cargado y contiene las siguientes columnas\n",
        "categorical_columns = ['ipress', 'region', 'departamento', 'provincia','diagnosticos','sexo','tipo_seguro']\n",
        "numeric_columns = ['EDAD']\n",
        "target_column = 'MONTO_BRUTO'\n",
        "\n",
        "# Transformar variables categóricas a numéricas\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_columns]\n",
        "\n",
        "# Crear vector de características\n",
        "feature_columns = [col + \"_index\" for col in categorical_columns] + numeric_columns\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Definir pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "df_transformed = pipeline.fit(df).transform(df)\n",
        "df_transformed = df_transformed.select(\"features\", target_column)\n",
        "\n",
        "# Dividir en datos de entrenamiento y prueba\n",
        "train_data, test_data = df_transformed.randomSplit([0.7, 0.3], seed=1234)\n",
        "\n",
        "# Definir modelos\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=target_column),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=target_column, maxBins=450),\n",
        "    \"Random Forest\": RandomForestRegressor(featuresCol=\"features\", labelCol=target_column, numTrees=100, maxBins=450),\n",
        "    \"Gradient Boosting\": GBTRegressor(featuresCol=\"features\", labelCol=target_column, maxBins=450)\n",
        "}\n",
        "\n",
        "# Evaluadores\n",
        "rmse_evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "r2_evaluator = RegressionEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "# Entrenar y evaluar modelos\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    trained_model = model.fit(train_data)\n",
        "    predictions = trained_model.transform(test_data)\n",
        "    rmse = rmse_evaluator.evaluate(predictions)\n",
        "    r2 = r2_evaluator.evaluate(predictions)\n",
        "    results[name] = {\"RMSE\": rmse, \"R²\": r2}\n",
        "\n",
        "# Mostrar resultados de los cuatro modelos\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name} -> RMSE: {metrics['RMSE']:.4f}, R²: {metrics['R²']:.4f}\")\n",
        "\n",
        "# Identificar el mejor modelo\n",
        "best_model = min(results, key=lambda x: results[x][\"RMSE\"])\n",
        "print(f\"\\nEl mejor modelo es {best_model} con RMSE: {results[best_model]['RMSE']:.4f} y R²: {results[best_model]['R²']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Eliminar columnas con un solo valor único\n",
        "df = df.drop('grupo_diagnosticos', 'periodo', 'grupo_cobertura', 'FECHA_CORTE_VALID', 'FECHA_ATENCION_VALID', 'fecha_corte', 'fecha_atencion')\n",
        "\n",
        "# 1. Indexar variables categóricas\n",
        "categorical_columns = ['ipress','region', 'departamento', 'provincia', 'diagnosticos', 'sexo', 'tipo_seguro']\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
        "\n",
        "# 2. Crear el vector de características\n",
        "feature_columns = [col + \"_index\" for col in categorical_columns] + ['EDAD', 'mes']\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# 3. Escalar las características\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# 4. Definir la variable objetivo\n",
        "df = df.withColumnRenamed(\"MONTO_BRUTO\", \"label\")\n",
        "\n",
        "# 5. Construir el pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
        "df_transformed = pipeline.fit(df).transform(df)\n",
        "\n",
        "# 6. Seleccionar solo las columnas necesarias\n",
        "df_transformed = df_transformed.select(\"scaledFeatures\", \"label\")\n",
        "\n",
        "# 7. Dividir en entrenamiento y prueba\n",
        "train_data, test_data = df_transformed.randomSplit([0.7, 0.3], seed=1234)\n",
        "\n",
        "# 8. Modelos de regresión con ajuste de hiperparámetros\n",
        "models = {\n",
        "    \"Regresión Lineal\": LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\", regParam=0.1, elasticNetParam=0.8),\n",
        "    \"Árbol de Decisión\": DecisionTreeRegressor(featuresCol=\"scaledFeatures\", labelCol=\"label\", maxDepth=8, maxBins=500),\n",
        "    \"Random Forest\": RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=\"label\", numTrees=200, maxDepth=5, maxBins=500),\n",
        "    \"Gradient Boosting\": GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=\"label\", maxIter=100, maxDepth=5, maxBins=500)\n",
        "}\n",
        "\n",
        "# Evaluador de regresión\n",
        "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# 9. Entrenamiento y evaluación\n",
        "for name, model in models.items():\n",
        "    print(f\"Entrenando {name}...\")\n",
        "    model_fit = model.fit(train_data)\n",
        "    predictions = model_fit.transform(test_data)\n",
        "    \n",
        "    # Evaluar RMSE y R²\n",
        "    rmse = evaluator.evaluate(predictions)\n",
        "    evaluator.setMetricName(\"r2\")\n",
        "    r2 = evaluator.evaluate(predictions)\n",
        "    \n",
        "    print(f\"{name} -> RMSE: {rmse}, R²: {r2}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#7. PERO POR LO MENOS ESTO SI TENEMOS QUE HACER\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#8. MODELO QUE VA A PREDECIR\n",
        "# Y ACA RECIEN TENEMOS QUE HACER EL MODELO PREDICTIVO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "Enfermedad"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
